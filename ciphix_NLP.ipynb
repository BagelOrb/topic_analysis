{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa60414-fab0-4356-9c7d-4e149975e173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T06:57:35.207651Z",
     "iopub.status.busy": "2023-03-01T06:57:35.207530Z",
     "iopub.status.idle": "2023-03-01T06:57:35.656079Z",
     "shell.execute_reply": "2023-03-01T06:57:35.655472Z",
     "shell.execute_reply.started": "2023-03-01T06:57:35.207639Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sanitization\n",
    "\n",
    "\n",
    "# Read the CSV file and extract the text data\n",
    "with open('ciphix NLP/translated_data.csv', 'r') as file:\n",
    "    text_ = file.readlines()\n",
    "\n",
    "## temporarily use less than the full data set for faster computation\n",
    "lines = text_[0:len(text_) // 10]\n",
    "len(lines)\n",
    "\n",
    "tokens = sanitization.sanitize_tokenize(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da555902-91bf-49b1-a440-699f4a7bbabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T06:57:04.886521Z",
     "iopub.status.busy": "2023-03-01T06:57:04.886178Z",
     "iopub.status.idle": "2023-03-01T06:57:04.889441Z",
     "shell.execute_reply": "2023-03-01T06:57:04.888913Z",
     "shell.execute_reply.started": "2023-03-01T06:57:04.886503Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello False\n",
      "you True\n",
      "to True\n",
      "why True\n",
      "in True\n",
      "is True\n",
      "was True\n",
      "go False\n",
      "say False\n",
      "yes False\n"
     ]
    }
   ],
   "source": [
    "for word in ['hello', 'you', 'to', 'why', 'in', 'is', 'was', 'go', 'say', 'yes']:\n",
    "    print(word, word in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de5df1c-3d5e-452d-8fb2-dd188aea1f58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T06:57:48.617937Z",
     "iopub.status.busy": "2023-03-01T06:57:48.617750Z",
     "iopub.status.idle": "2023-03-01T06:58:00.438098Z",
     "shell.execute_reply": "2023-03-01T06:58:00.437610Z",
     "shell.execute_reply.started": "2023-03-01T06:57:48.617924Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.074*\"time\" + 0.039*\"delivery\" + 0.029*\"available\" + 0.027*\"pay\" + 0.023*\"show\" + 0.019*\"every\" + 0.018*\"first\" + 0.017*\"ty\" + 0.016*\"home\"')\n",
      "(1, '0.037*\"store\" + 0.037*\"connect\" + 0.031*\"currently\" + 0.026*\"io\" + 0.022*\"connection\" + 0.022*\"set\" + 0.017*\"leave\" + 0.016*\"local\" + 0.015*\"im\"')\n",
      "(2, '0.138*\"thanks\" + 0.043*\"info\" + 0.030*\"hi\" + 0.029*\"feedback\" + 0.028*\"update\" + 0.023*\"working\" + 0.023*\"appreciate\" + 0.021*\"sure\" + 0.019*\"soon\"')\n",
      "(3, '0.036*\"offer\" + 0.036*\"point\" + 0.029*\"another\" + 0.025*\"nothing\" + 0.023*\"stopping\" + 0.022*\"told\" + 0.020*\"stop\" + 0.020*\"get\" + 0.020*\"cost\"')\n",
      "(4, '0.046*\"issue\" + 0.041*\"halloween\" + 0.037*\"happy\" + 0.028*\"hello\" + 0.027*\"would\" + 0.026*\"assistance\" + 0.024*\"sorry\" + 0.021*\"inconvenience\" + 0.021*\"idea\"')\n",
      "(5, '0.087*\"u\" + 0.071*\"help\" + 0.058*\"know\" + 0.053*\"let\" + 0.041*\"please\" + 0.036*\"sorry\" + 0.030*\"hear\" + 0.028*\"hey\" + 0.018*\"like\"')\n",
      "(6, '0.084*\"please\" + 0.076*\"u\" + 0.054*\"look\" + 0.035*\"email\" + 0.033*\"address\" + 0.030*\"send\" + 0.029*\"take\" + 0.027*\"help\" + 0.027*\"hi\"')\n",
      "(7, '0.146*\"account\" + 0.033*\"package\" + 0.027*\"seller\" + 0.023*\"live\" + 0.020*\"hope\" + 0.018*\"watch\" + 0.018*\"business\" + 0.017*\"help\" + 0.016*\"id\"')\n",
      "(8, '0.053*\"contact\" + 0.044*\"number\" + 0.039*\"iphone\" + 0.038*\"experience\" + 0.030*\"understand\" + 0.027*\"getting\" + 0.026*\"made\" + 0.021*\"well\" + 0.020*\"better\"')\n",
      "(9, '0.073*\"one\" + 0.048*\"sent\" + 0.028*\"information\" + 0.021*\"console\" + 0.021*\"actually\" + 0.019*\"xbox\" + 0.018*\"service\" + 0.017*\"ordered\" + 0.017*\"worst\"')\n",
      "(10, '0.069*\"service\" + 0.049*\"customer\" + 0.037*\"phone\" + 0.023*\"internet\" + 0.023*\"guy\" + 0.018*\"call\" + 0.018*\"ca\" + 0.018*\"give\" + 0.017*\"people\"')\n",
      "(11, '0.023*\"called\" + 0.022*\"way\" + 0.021*\"last\" + 0.020*\"setting\" + 0.019*\"already\" + 0.017*\"mean\" + 0.016*\"answer\" + 0.016*\"change\" + 0.015*\"half\"')\n",
      "(12, '0.032*\"try\" + 0.030*\"work\" + 0.030*\"problem\" + 0.030*\"still\" + 0.026*\"app\" + 0.019*\"find\" + 0.018*\"get\" + 0.017*\"want\" + 0.015*\"ok\"')\n",
      "(13, '0.074*\"via\" + 0.054*\"link\" + 0.045*\"much\" + 0.032*\"following\" + 0.028*\"twitter\" + 0.024*\"check\" + 0.023*\"tracking\" + 0.019*\"money\" + 0.019*\"real\"')\n",
      "(14, '0.040*\"see\" + 0.033*\"able\" + 0.026*\"also\" + 0.021*\"kind\" + 0.021*\"want\" + 0.017*\"test\" + 0.016*\"yet\" + 0.016*\"gate\" + 0.014*\"still\"')\n",
      "(15, '0.040*\"great\" + 0.037*\"item\" + 0.028*\"delay\" + 0.025*\"fix\" + 0.023*\"product\" + 0.016*\"additional\" + 0.015*\"post\" + 0.015*\"afc\" + 0.015*\"start\"')\n",
      "(16, '0.061*\"flight\" + 0.032*\"back\" + 0.024*\"day\" + 0.023*\"year\" + 0.021*\"next\" + 0.016*\"get\" + 0.014*\"come\" + 0.014*\"free\" + 0.013*\"support\"')\n",
      "(17, '0.051*\"order\" + 0.040*\"say\" + 0.032*\"get\" + 0.031*\"card\" + 0.028*\"costume\" + 0.028*\"burrito\" + 0.025*\"bowl\" + 0.023*\"trying\" + 0.023*\"boorito\"')\n",
      "(18, '0.082*\"go\" + 0.029*\"minute\" + 0.027*\"welcome\" + 0.023*\"fixed\" + 0.023*\"driver\" + 0.022*\"month\" + 0.021*\"continue\" + 0.020*\"battery\" + 0.019*\"refund\"')\n",
      "(19, '0.099*\"thank\" + 0.040*\"yes\" + 0.038*\"got\" + 0.033*\"good\" + 0.025*\"reply\" + 0.022*\"seat\" + 0.020*\"best\" + 0.020*\"friend\" + 0.020*\"long\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Create a dictionary of the tokens\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "\n",
    "# Create a corpus of the tokens\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "# Train an LDA model on the corpus\n",
    "lda_model = models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                     id2word=dictionary,\n",
    "                                     num_topics=20,\n",
    "                                     passes=40,\n",
    "                                     alpha='auto',\n",
    "                                     random_state=42)\n",
    "\n",
    "# Print the most prevalent topics discussed in the text data\n",
    "topics = lda_model.print_topics(num_words=9)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    \n",
    "# Findings:\n",
    "#  sometimes gives boorito, sometimes not, depending on random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3a9b37-9cea-4ffb-8225-ace80a05080f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T07:25:11.749550Z",
     "iopub.status.busy": "2023-02-28T07:25:11.749442Z",
     "iopub.status.idle": "2023-02-28T07:25:11.770928Z",
     "shell.execute_reply": "2023-02-28T07:25:11.770501Z",
     "shell.execute_reply.started": "2023-02-28T07:25:11.749540Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminology:\n",
      "[('let', 'know'), ('email', 'address'), ('sorry', 'hear'), ('please', 'send'), ('customer', 'service'), ('take', 'look'), ('please', 'let'), ('direct', 'message'), ('help', 'send'), ('phone', 'number'), ('happy', 'help'), ('would', 'like'), ('help', 'please'), ('please', 'contact'), ('please', 'follow'), ('send', 'email'), ('send', 'note'), ('make', 'sure'), ('please', 'check'), ('look', 'like'), ('please', 'reach'), ('happy', 'halloween'), ('please', 'help'), ('need', 'help'), ('closer', 'look'), ('take', 'closer'), ('account', 'email'), ('anything', 'else'), ('could', 'help'), ('look', 'please'), ('need', 'assistance'), ('team', 'connect'), ('thanks', 'reaching'), ('contact', 'number'), ('hear', 'please'), ('please', 'provide'), ('confirmation', 'number'), ('could', 'please'), ('get', 'back'), ('help', 'hi')]\n"
     ]
    }
   ],
   "source": [
    "# Extract bigrams; word pairs which often go together\n",
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "words = list(itertools.chain.from_iterable(tokens))  # concatenate list of lists into single list\n",
    "\n",
    "# Find the most common bigrams in the text\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "finder.apply_freq_filter(3)\n",
    "bigrams = finder.nbest(bigram_measures.raw_freq, 40)\n",
    "\n",
    "# Print the extracted terminology\n",
    "print(\"Terminology:\")\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a86e68a4-e74e-4f0a-8c18-bd292591ec1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T07:36:30.396411Z",
     "iopub.status.busy": "2023-02-28T07:36:30.396248Z",
     "iopub.status.idle": "2023-02-28T07:36:30.427302Z",
     "shell.execute_reply": "2023-02-28T07:36:30.426530Z",
     "shell.execute_reply.started": "2023-02-28T07:36:30.396399Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1: \"understand would like assist would need get private secured link assist\" TF-IDF Vector:\n",
      "\n",
      "link 0.23162194562021757\n",
      "secured 0.3689624733283365\n",
      "private 0.3118269598581018\n",
      "get 0.18865171494823785\n",
      "need 0.21390621720424144\n",
      "assist 0.47064827907513135\n",
      "like 0.19525074664453232\n",
      "would 0.4258537746485734\n",
      "understand 0.28632450217497574\n",
      "115712 0.32965248487891047\n",
      "\n",
      "\n",
      "Tweet 2: \"propose\" TF-IDF Vector:\n",
      "\n",
      "propose 0.8149003051737236\n",
      "sprintcare 0.5796011496087392\n",
      "\n",
      "\n",
      "Tweet 3: \"sent several private message one responding usual\" TF-IDF Vector:\n",
      "\n",
      "usual 0.4389380444777185\n",
      "responding 0.391855012156571\n",
      "one 0.25182262048069975\n",
      "messages 0.37203513688161577\n",
      "several 0.37203513688161577\n",
      "sent 0.2793632468586064\n",
      "sprintcare 0.3282217801891854\n",
      "private 0.35745596027636406\n",
      "\n",
      "\n",
      "Tweet 4: \"please send private message assist click message top profile\" TF-IDF Vector:\n",
      "\n",
      "profile 0.35595395911300093\n",
      "top 0.3498938268461945\n",
      "click 0.3350921077350238\n",
      "message 0.49954705339067396\n",
      "us 0.15265592540593806\n",
      "send 0.21662029096312777\n",
      "please 0.1563738512740977\n",
      "private 0.33097377785167725\n",
      "assist 0.24977352669533698\n",
      "115712 0.3498938268461945\n",
      "\n",
      "\n",
      "Tweet 5: \"\" TF-IDF Vector:\n",
      "\n",
      "sprintcare 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into TF-IDF vectors\n",
    "tfidf_vectors = vectorizer.fit_transform(text)\n",
    "\n",
    "import scipy \n",
    "\n",
    "inverse_voc = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "\n",
    "# Print the TF-IDF vectors\n",
    "for i in range(5):\n",
    "    print(f\"Tweet {i+1}: \\\"{' '.join(tokens[i])}\\\" TF-IDF Vector:\\n\")\n",
    "    cx = scipy.sparse.coo_matrix(tfidf_vectors[i])\n",
    "    for i, j, v in zip(cx.row, cx.col, cx.data):\n",
    "        print(inverse_voc[j], v)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ce087-86be-41e9-9659-9132c142dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Get pre-trained Word2Vec transformation to the input words\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "befcddfc-892b-47f8-891f-bd25534c62fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T08:06:17.087236Z",
     "iopub.status.busy": "2023-02-28T08:06:17.087063Z",
     "iopub.status.idle": "2023-02-28T08:06:17.179243Z",
     "shell.execute_reply": "2023-02-28T08:06:17.178589Z",
     "shell.execute_reply.started": "2023-02-28T08:06:17.087222Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define the Word2Vec model parameters\n",
    "word2vec = Word2Vec(tokens, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d301514-32cf-4612-bf1d-a9b3a075986e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T07:46:55.023181Z",
     "iopub.status.busy": "2023-02-28T07:46:55.023023Z",
     "iopub.status.idle": "2023-02-28T07:46:57.244764Z",
     "shell.execute_reply": "2023-02-28T07:46:57.244369Z",
     "shell.execute_reply.started": "2023-02-28T07:46:55.023169Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weighted K-means clustering on word2vecs weighted by TF-IDF score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import itertools\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "words = list(itertools.chain.from_iterable(tokens))  # concatenate list of lists into single list\n",
    "\n",
    "\n",
    "# Create the TF-IDF vectorizer and Fit it to the all tweets as one document\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
    "tfidf_vectors = vectorizer.fit_transform([' '.join(words)])\n",
    "\n",
    "# Get the vocabulary and the TF-IDF weights\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "weights = np.squeeze(tfidf_vectors.toarray())\n",
    "\n",
    "\n",
    "word_vecs = [word2vec.wv[word] for word in vocabulary.keys()]\n",
    "\n",
    "# Define the number of clusters\n",
    "num_clusters = 10\n",
    "\n",
    "# Create the KMeans object and fit it to the TF-IDF vectors\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(word_vecs, sample_weight=weights)\n",
    "\n",
    "# Get the cluster labels and the cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=10)\n",
    "neigh.fit(word_vecs)\n",
    "\n",
    "# Print the top words of each cluster, weighted by their TF-IDF score\n",
    "for i, center in enumerate(centers):\n",
    "    print(\"Cluster\", i+1, \"Top Words:\")\n",
    "    [dists], [indices] = neigh.kneighbors([center])\n",
    "    for word_index in indices:\n",
    "        word_weighted_score = weights[word_index]\n",
    "        word = word2vec.wv.index_to_key[word_index]\n",
    "        print(f\"\\t{word} (score: {word_weighted_score:.2e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62f4e6-b5ce-4f5b-8b1e-9333e55dbbf6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T07:25:36.662221Z",
     "iopub.status.idle": "2023-02-28T07:25:36.662391Z",
     "shell.execute_reply": "2023-02-28T07:25:36.662336Z",
     "shell.execute_reply.started": "2023-02-28T07:25:36.662330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997df553-96ce-41a7-aed7-9d9d13bee451",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T07:25:36.662873Z",
     "iopub.status.idle": "2023-02-28T07:25:36.663345Z",
     "shell.execute_reply": "2023-02-28T07:25:36.663232Z",
     "shell.execute_reply.started": "2023-02-28T07:25:36.663219Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for line in tokens[0:30]:\n",
    "    print(' '.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa84fe8-452b-4c2b-89c7-e02ecb63143f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T07:25:36.663795Z",
     "iopub.status.idle": "2023-02-28T07:25:36.664089Z",
     "shell.execute_reply": "2023-02-28T07:25:36.663998Z",
     "shell.execute_reply.started": "2023-02-28T07:25:36.663987Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, line in enumerate(tokens):\n",
    "    if 'wa' in line:\n",
    "        print(text[i])\n",
    "        print(' '.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328e745-9038-4543-8937-b2d855b6c9ca",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T07:25:36.664538Z",
     "iopub.status.idle": "2023-02-28T07:25:36.664790Z",
     "shell.execute_reply": "2023-02-28T07:25:36.664699Z",
     "shell.execute_reply.started": "2023-02-28T07:25:36.664689Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('was')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b64e5-2473-4ba5-8bf9-ba4394688ffc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T07:25:36.665235Z",
     "iopub.status.idle": "2023-02-28T07:25:36.665468Z",
     "shell.execute_reply": "2023-02-28T07:25:36.665380Z",
     "shell.execute_reply.started": "2023-02-28T07:25:36.665370Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer.fit_transform([\"you wasted wasted\"]).toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
