{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae1b77b-4ca7-4b27-b54d-236148f0ed63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T13:07:58.745027Z",
     "iopub.status.busy": "2023-03-01T13:07:58.744790Z",
     "iopub.status.idle": "2023-03-01T13:07:58.747451Z",
     "shell.execute_reply": "2023-03-01T13:07:58.747039Z",
     "shell.execute_reply.started": "2023-03-01T13:07:58.745009Z"
    },
    "tags": []
   },
   "source": [
    "# Load and clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd037708-7c36-4c49-91e2-2a40fc3a9b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T07:14:17.505068Z",
     "iopub.status.busy": "2023-03-04T07:14:17.504883Z",
     "iopub.status.idle": "2023-03-04T07:14:18.919890Z",
     "shell.execute_reply": "2023-03-04T07:14:18.919512Z",
     "shell.execute_reply.started": "2023-03-04T07:14:17.505053Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264435\n"
     ]
    }
   ],
   "source": [
    "import sanitization\n",
    "\n",
    "\n",
    "# Read the CSV file and extract the text data\n",
    "# Downloaded from https://ciphix.io/ai/data.csv\n",
    "with open('ciphix NLP/untranslated_data.csv', 'r') as file:\n",
    "    lines_ = file.readlines()\n",
    "\n",
    "## temporarily use less than the full data set for faster computation\n",
    "lines = lines_[0:len(lines_) // 10]\n",
    "#lines = lines_\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a90541a-91e4-4adf-9460-02a958f5e763",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T03:26:30.009436Z",
     "iopub.status.busy": "2023-03-04T03:26:30.009242Z",
     "iopub.status.idle": "2023-03-04T03:48:38.713945Z",
     "shell.execute_reply": "2023-03-04T03:48:38.713425Z",
     "shell.execute_reply.started": "2023-03-04T03:26:30.009424Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tokens = sanitization.sanitize_tokenize(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d973172c-2fc0-4f73-9258-5df7c5300e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T03:49:46.453524Z",
     "iopub.status.busy": "2023-03-04T03:49:46.453330Z",
     "iopub.status.idle": "2023-03-04T03:49:51.935102Z",
     "shell.execute_reply": "2023-03-04T03:49:51.934459Z",
     "shell.execute_reply.started": "2023-03-04T03:49:46.453508Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ciphix NLP/tokens.pickle', 'wb') as file:\n",
    "    pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4447165c-bce3-4cad-92cb-d76646709b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T07:16:50.160076Z",
     "iopub.status.busy": "2023-03-04T07:16:50.159923Z",
     "iopub.status.idle": "2023-03-04T07:16:50.600060Z",
     "shell.execute_reply": "2023-03-04T07:16:50.599313Z",
     "shell.execute_reply.started": "2023-03-04T07:16:50.160063Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ciphix NLP/tokens.pickle', 'rb') as file:\n",
    "    tokens = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcdae3-ac8d-47c0-9e0f-ca5ee7df6b20",
   "metadata": {},
   "source": [
    "# Specificity score\n",
    "\n",
    "Make a score for each word based on how often it occurs in general speech, i.e. outside our current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fedf305d-eb64-442c-8c3a-cdc4c3895150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T05:52:52.823825Z",
     "iopub.status.busy": "2023-03-04T05:52:52.823630Z",
     "iopub.status.idle": "2023-03-04T06:04:30.119802Z",
     "shell.execute_reply": "2023-03-04T06:04:30.119270Z",
     "shell.execute_reply.started": "2023-03-04T05:52:52.823809Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/tk/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/tk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/tk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Make a global corpus from Brown\n",
    "\n",
    "import nltk\n",
    "import gensim.models\n",
    "from nltk.corpus import brown\n",
    "\n",
    "try:\n",
    "    nltk.data.find('brown')\n",
    "except LookupError:\n",
    "    nltk.download('brown')\n",
    "\n",
    "global_tokens = sanitization.sanitize_tokenize(brown.words()) # Use brown corpus as single tweet\n",
    "\n",
    "# Add random twitter data to the curpus\n",
    "\n",
    "import csv\n",
    "\n",
    "# Read the CSV file and extract the text data\n",
    "lines = []\n",
    "#  Downloaded from https://www.kaggle.com/datasets/kazanova/sentiment140/discussion/60512\n",
    "with open('ciphix NLP/training.1600000.processed.noemoticon.csv', 'r', encoding='ISO-8859-1') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        lines.append(line[5])  # Column 5 is the actual tweet\n",
    "        \n",
    "rnd_twitter_tokens = sanitization.sanitize_tokenize(lines) \n",
    "\n",
    "global_tokens.extend(rnd_twitter_tokens)\n",
    "\n",
    "# Add our tokens to the set so we don't get problems with unknown tokens\n",
    "global_tokens.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac1d94ea-03df-4aa7-942d-a29cd1b3c53c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T06:07:48.181630Z",
     "iopub.status.busy": "2023-03-04T06:07:48.181467Z",
     "iopub.status.idle": "2023-03-04T06:07:55.693758Z",
     "shell.execute_reply": "2023-03-04T06:07:55.693258Z",
     "shell.execute_reply.started": "2023-03-04T06:07:48.181618Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ciphix NLP/global_tokens.pickle', 'wb') as file:\n",
    "    pickle.dump(global_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa4e962-1910-476f-9171-4e6a79ee68d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T07:16:57.650725Z",
     "iopub.status.busy": "2023-03-04T07:16:57.650553Z",
     "iopub.status.idle": "2023-03-04T07:17:06.390660Z",
     "shell.execute_reply": "2023-03-04T07:17:06.389982Z",
     "shell.execute_reply.started": "2023-03-04T07:16:57.650712Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ciphix NLP/global_tokens.pickle', 'rb') as file:\n",
    "    global_tokens = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cb27b5c-85a4-4c71-ad47-a5bdc1b05238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T06:08:01.789803Z",
     "iopub.status.busy": "2023-03-04T06:08:01.789620Z",
     "iopub.status.idle": "2023-03-04T06:08:10.987270Z",
     "shell.execute_reply": "2023-03-04T06:08:10.986804Z",
     "shell.execute_reply.started": "2023-03-04T06:08:01.789792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Create a frequency distribution of all words from a different context then our current dataset\n",
    "global_fdist = FreqDist()\n",
    "for line in global_tokens:\n",
    "    for word in line:\n",
    "        global_fdist[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b25a8c-fe38-4f48-891e-ac9785c5495c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "assert(len(global_fdist))  # Only save when we have data\n",
    "\n",
    "with open('ciphix NLP/freqs.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for word in global_fdist:\n",
    "        writer.writerow([word, global_fdist.freq(word)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc80a9d-820c-4fd6-975c-3809f07f21f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T07:17:25.198981Z",
     "iopub.status.busy": "2023-03-04T07:17:25.198808Z",
     "iopub.status.idle": "2023-03-04T07:17:25.405799Z",
     "shell.execute_reply": "2023-03-04T07:17:25.405412Z",
     "shell.execute_reply.started": "2023-03-04T07:17:25.198968Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Create mapping from words to their frequency in 'normal language'\n",
    "word2freq_dict = {}\n",
    "\n",
    "with open('ciphix NLP/freqs.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for [word, freq] in reader:\n",
    "        word2freq_dict[word] = float(freq)\n",
    "        \n",
    "assert(len(word2freq_dict))  # We should have loaded actual data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "483da75f-66ec-4405-92bc-d0611653283b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T09:41:57.965666Z",
     "iopub.status.busy": "2023-03-03T09:41:57.965444Z",
     "iopub.status.idle": "2023-03-03T09:41:58.578491Z",
     "shell.execute_reply": "2023-03-03T09:41:58.577934Z",
     "shell.execute_reply.started": "2023-03-03T09:41:57.965651Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "local_fdist = FreqDist()\n",
    "for line in tokens:\n",
    "    for word in line:\n",
    "        local_fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4fbdc9a-064d-4b1e-9d35-d286d4124bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T07:17:50.595080Z",
     "iopub.status.busy": "2023-03-04T07:17:50.594925Z",
     "iopub.status.idle": "2023-03-04T07:17:51.018506Z",
     "shell.execute_reply": "2023-03-04T07:17:51.017618Z",
     "shell.execute_reply.started": "2023-03-04T07:17:50.595066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce \n",
    "\n",
    "global_size = reduce(lambda count, l: count + len(l), global_tokens, 0)\n",
    "\n",
    "local_size = reduce(lambda count, l: count + len(l), tokens, 0)\n",
    "\n",
    "def word2freq(word):\n",
    "    try: \n",
    "        return global_fdist[word]\n",
    "        #return local_fdist[word]\n",
    "    except KeyError:  # word didn't occur in our dataset\n",
    "        return 1. / global_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714d25c-6177-491d-b0af-67780cbc9a44",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de5df1c-3d5e-452d-8fb2-dd188aea1f58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T07:17:53.153562Z",
     "iopub.status.busy": "2023-03-04T07:17:53.153391Z",
     "iopub.status.idle": "2023-03-04T08:07:33.891401Z",
     "shell.execute_reply": "2023-03-04T08:07:33.887616Z",
     "shell.execute_reply.started": "2023-03-04T07:17:53.153549Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train an LDA model on the corpus\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mldamodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLdaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_passes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Print the most prevalent topics discussed in the text data\u001b[39;00m\n\u001b[1;32m     24\u001b[0m lda_model\u001b[38;5;241m.\u001b[39mprint_topics(num_topics\u001b[38;5;241m=\u001b[39mnum_top_topics, num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gensim/models/ldamodel.py:521\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    519\u001b[0m use_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    520\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_as_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    524\u001b[0m     msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    525\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gensim/models/ldamodel.py:1020\u001b[0m, in \u001b[0;36mLdaModel.update\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreached the end of input; now waiting for all remaining jobs to finish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1019\u001b[0m     other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher\u001b[38;5;241m.\u001b[39mgetstate()\n\u001b[0;32m-> 1020\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrho\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m other  \u001b[38;5;66;03m# frees up memory\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gensim/models/ldamodel.py:1066\u001b[0m, in \u001b[0;36mLdaModel.do_mstep\u001b[0;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[1;32m   1063\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdating topics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# update self with the new blend; also keep track of how much did\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# the topics change through this update, to assess convergence\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m previous_Elogbeta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_Elogbeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mblend(rho, other)\n\u001b[1;32m   1069\u001b[0m current_Elogbeta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_Elogbeta()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gensim/models/ldamodel.py:282\u001b[0m, in \u001b[0;36mLdaState.get_Elogbeta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_Elogbeta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    275\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the log (posterior) probabilities for each topic.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m        Posterior probabilities for each topic.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdirichlet_expectation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "num_topics = 100\n",
    "num_top_topics = 10\n",
    "num_passes = 3\n",
    "\n",
    "# Create a dictionary of the tokens\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "\n",
    "# Create a corpus of the tokens\n",
    "corpus = [dictionary.doc2bow(line) for line in tokens]\n",
    "\n",
    "# Train an LDA model on the corpus\n",
    "lda_model = models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                     id2word=dictionary,\n",
    "                                     num_topics=num_topics,\n",
    "                                     passes=num_passes,\n",
    "                                     alpha='auto',\n",
    "                                     random_state=42\n",
    "                                    )\n",
    "\n",
    "# Print the most prevalent topics discussed in the text data\n",
    "lda_model.print_topics(num_topics=num_top_topics, num_words=8)\n",
    "    \n",
    "# Findings:\n",
    "#  Sometimes gives boorito, sometimes not, depending on random_state,`\n",
    "#  even though I think that term is highly specific and would be a good candidate for a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0328912-6dc9-4df8-a16c-fdbdc30a7673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T06:15:55.361493Z",
     "iopub.status.busy": "2023-03-04T06:15:55.361223Z",
     "iopub.status.idle": "2023-03-04T06:15:55.548258Z",
     "shell.execute_reply": "2023-03-04T06:15:55.547686Z",
     "shell.execute_reply.started": "2023-03-04T06:15:55.361477Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import corpora\n",
    "import os\n",
    "\n",
    "# Save model to disk.\n",
    "file = datapath(os.getcwd() + \"/ciphix NLP/lda_model.lda\")\n",
    "lda_model.save(file)\n",
    "\n",
    "\n",
    "file = datapath(os.getcwd() + \"/ciphix NLP/dictionary.pickle\")\n",
    "dictionary.save(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d50d20e-3a72-4b58-89b4-af5b87595c3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T03:59:23.629357Z",
     "iopub.status.busy": "2023-03-04T03:59:23.629160Z",
     "iopub.status.idle": "2023-03-04T03:59:23.933968Z",
     "shell.execute_reply": "2023-03-04T03:59:23.933517Z",
     "shell.execute_reply.started": "2023-03-04T03:59:23.629342Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import os\n",
    "\n",
    "# Load model from disk.\n",
    "file = datapath(os.getcwd() + \"/ciphix NLP/lda_model.lda\")\n",
    "lda_model = LdaModel.load(file)\n",
    "\n",
    "\n",
    "file = datapath(os.getcwd() + \"/ciphix NLP/dictionary.pickle\")\n",
    "dictionary = corpora.Dictionary.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2590edf-d4c6-430f-bb06-6000a4d458ce",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-04T05:23:10.509541Z",
     "iopub.status.busy": "2023-03-04T05:23:10.509382Z",
     "iopub.status.idle": "2023-03-04T05:23:21.967711Z",
     "shell.execute_reply": "2023-03-04T05:23:21.967255Z",
     "shell.execute_reply.started": "2023-03-04T05:23:10.509530Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (-1, 0),\n",
       " 1: (1286, 0.35301313),\n",
       " 2: (1940, 0.30972847),\n",
       " 3: (-1, 0),\n",
       " 4: (1692, 0.23932157),\n",
       " 5: (644, 0.28705502),\n",
       " 6: (-1, 0),\n",
       " 7: (-1, 0),\n",
       " 8: (-1, 0),\n",
       " 9: (558, 0.36593404),\n",
       " 10: (1694, 0.27029648),\n",
       " 11: (-1, 0),\n",
       " 12: (1904, 0.26082495),\n",
       " 13: (1734, 0.3254792),\n",
       " 14: (1866, 0.33220857),\n",
       " 15: (1659, 0.28760242),\n",
       " 16: (1641, 0.25386384),\n",
       " 17: (-1, 0),\n",
       " 18: (1765, 0.2545081),\n",
       " 19: (17321, 0.1721748),\n",
       " 20: (1688, 0.23882107),\n",
       " 21: (234, 0.36048797),\n",
       " 22: (641, 0.30806866),\n",
       " 23: (86007, 0.22822103),\n",
       " 24: (1398, 0.4296613),\n",
       " 25: (1753, 0.34911054),\n",
       " 26: (319, 0.38230428),\n",
       " 27: (1695, 0.14855479),\n",
       " 28: (-1, 0),\n",
       " 29: (181, 0.21644239),\n",
       " 30: (-1, 0),\n",
       " 31: (1032, 0.44334382),\n",
       " 32: (1650, 0.38540772),\n",
       " 33: (626, 0.32067284),\n",
       " 34: (1503, 0.39601025),\n",
       " 35: (1776, 0.27324727),\n",
       " 36: (-1, 0),\n",
       " 37: (480, 0.27711806),\n",
       " 38: (1047, 0.23139562),\n",
       " 39: (-1, 0),\n",
       " 40: (-1, 0),\n",
       " 41: (1306, 0.3041109),\n",
       " 42: (1660, 0.28152856),\n",
       " 43: (120, 0.34525943),\n",
       " 44: (-1, 0),\n",
       " 45: (219, 0.23801398),\n",
       " 46: (1165, 0.24713857),\n",
       " 47: (-1, 0),\n",
       " 48: (1868, 0.3238597),\n",
       " 49: (1082, 0.25790635),\n",
       " 50: (1849, 0.23232497),\n",
       " 51: (-1, 0),\n",
       " 52: (1945, 0.2952275),\n",
       " 53: (1308, 0.38020524),\n",
       " 54: (-1, 0),\n",
       " 55: (1957, 0.33168235),\n",
       " 56: (693, 0.30483007),\n",
       " 57: (-1, 0),\n",
       " 58: (1658, 0.26022556),\n",
       " 59: (-1, 0),\n",
       " 60: (422, 0.3184786),\n",
       " 61: (821, 0.28010648),\n",
       " 62: (-1, 0),\n",
       " 63: (-1, 0),\n",
       " 64: (664, 0.30807385),\n",
       " 65: (674, 0.29996827),\n",
       " 66: (1566, 0.39337385),\n",
       " 67: (1955, 0.31862956),\n",
       " 68: (-1, 0),\n",
       " 69: (-1, 0),\n",
       " 70: (1298, 0.23499674),\n",
       " 71: (-1, 0),\n",
       " 72: (-1, 0),\n",
       " 73: (1627, 0.35455367),\n",
       " 74: (-1, 0),\n",
       " 75: (1692, 0.15610293),\n",
       " 76: (1685, 0.34596604),\n",
       " 77: (-1, 0),\n",
       " 78: (-1, 0),\n",
       " 79: (-1, 0),\n",
       " 80: (-1, 0),\n",
       " 81: (238, 0.34985256),\n",
       " 82: (527, 0.3932204),\n",
       " 83: (-1, 0),\n",
       " 84: (-1, 0),\n",
       " 85: (665, 0.25544402),\n",
       " 86: (1574, 0.2721426),\n",
       " 87: (-1, 0),\n",
       " 88: (1347, 0.35122457),\n",
       " 89: (-1, 0),\n",
       " 90: (-1, 0),\n",
       " 91: (-1, 0),\n",
       " 92: (-1, 0),\n",
       " 93: (121, 0.3640604),\n",
       " 94: (1695, 0.17424144),\n",
       " 95: (-1, 0),\n",
       " 96: (-1, 0),\n",
       " 97: (1623, 0.2753048),\n",
       " 98: (1984, 0.24952233),\n",
       " 99: (-1, 0)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find topic best matching a tweet\n",
    "def best_topic(line_tokens, lda_model):\n",
    "    topic_probs = lda_model.get_document_topics(dictionary.doc2bow(line_tokens))\n",
    "    return max(topic_probs, key=lambda pair: pair[1])\n",
    "\n",
    "# Find tweet best matching a topic\n",
    "def best_representatives(lda_model, num_tweets_to_check=100000):\n",
    "    first_topic_probs = lda_model.get_document_topics(bow = dictionary.doc2bow(tokens[0]))\n",
    "    best_topic_probs = { topic_id : (-1, 0) for topic_id in range(num_topics) }\n",
    "    for i in range(num_tweets_to_check):\n",
    "        line_tokens = tokens[i]\n",
    "        bow = dictionary.doc2bow(line_tokens)\n",
    "        topic_probs = lda_model.get_document_topics(bow)\n",
    "        for topic_id, topic_prob in topic_probs:\n",
    "            if topic_prob > best_topic_probs[topic_id][1]:\n",
    "                best_topic_probs[topic_id] = (i, topic_prob)\n",
    "    return best_topic_probs\n",
    "    \n",
    "\n",
    "representative_tweets = best_representatives(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bad79-2656-4c48-a416-926ad5cd712e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-04T06:57:08.052715Z",
     "iopub.status.idle": "2023-03-04T06:57:08.052842Z",
     "shell.execute_reply": "2023-03-04T06:57:08.052785Z",
     "shell.execute_reply.started": "2023-03-04T06:57:08.052778Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(bow = dictionary.doc2bow(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaf3e0eb-6c34-4aee-b60b-f3df0bbd3677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T04:02:49.569032Z",
     "iopub.status.busy": "2023-03-04T04:02:49.568825Z",
     "iopub.status.idle": "2023-03-04T04:02:49.828536Z",
     "shell.execute_reply": "2023-03-04T04:02:49.828111Z",
     "shell.execute_reply.started": "2023-03-04T04:02:49.569019Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(338,\n",
       "  '0.000*\"radiate\" + 0.000*\"unitedstates\" + 0.000*\"eliza\" + 0.000*\"takeofftips\" + 0.000*\"kevingray\" + 0.000*\"uncarrierexperience\" + 0.000*\"fitnessmotivation\" + 0.000*\"behealthy\" + 0.000*\"fitfam\" + 0.000*\"outwards\"'),\n",
       " (317,\n",
       "  '0.000*\"radiate\" + 0.000*\"unitedstates\" + 0.000*\"eliza\" + 0.000*\"takeofftips\" + 0.000*\"kevingray\" + 0.000*\"uncarrierexperience\" + 0.000*\"fitnessmotivation\" + 0.000*\"behealthy\" + 0.000*\"fitfam\" + 0.000*\"outwards\"'),\n",
       " (302,\n",
       "  '0.000*\"radiate\" + 0.000*\"unitedstates\" + 0.000*\"eliza\" + 0.000*\"takeofftips\" + 0.000*\"kevingray\" + 0.000*\"uncarrierexperience\" + 0.000*\"fitnessmotivation\" + 0.000*\"behealthy\" + 0.000*\"fitfam\" + 0.000*\"outwards\"'),\n",
       " (691,\n",
       "  '0.000*\"radiate\" + 0.000*\"unitedstates\" + 0.000*\"eliza\" + 0.000*\"takeofftips\" + 0.000*\"kevingray\" + 0.000*\"uncarrierexperience\" + 0.000*\"fitnessmotivation\" + 0.000*\"behealthy\" + 0.000*\"fitfam\" + 0.000*\"outwards\"'),\n",
       " (539,\n",
       "  '0.000*\"radiate\" + 0.000*\"unitedstates\" + 0.000*\"eliza\" + 0.000*\"takeofftips\" + 0.000*\"kevingray\" + 0.000*\"uncarrierexperience\" + 0.000*\"fitnessmotivation\" + 0.000*\"behealthy\" + 0.000*\"fitfam\" + 0.000*\"outwards\"'),\n",
       " (130,\n",
       "  '0.000*\"radiate\" + 0.000*\"unitedstates\" + 0.000*\"eliza\" + 0.000*\"takeofftips\" + 0.000*\"kevingray\" + 0.000*\"uncarrierexperience\" + 0.000*\"fitnessmotivation\" + 0.000*\"behealthy\" + 0.000*\"fitfam\" + 0.000*\"outwards\"'),\n",
       " (934,\n",
       "  '0.000*\"radiate\" + 0.000*\"unitedstates\" + 0.000*\"eliza\" + 0.000*\"takeofftips\" + 0.000*\"kevingray\" + 0.000*\"uncarrierexperience\" + 0.000*\"fitnessmotivation\" + 0.000*\"behealthy\" + 0.000*\"fitfam\" + 0.000*\"outwards\"'),\n",
       " (303,\n",
       "  '0.122*\"check\" + 0.109*\"take\" + 0.092*\"day\" + 0.065*\"message\" + 0.064*\"could\" + 0.060*\"tell\" + 0.056*\"ca\" + 0.040*\"really\" + 0.034*\"last\" + 0.033*\"error\"'),\n",
       " (770,\n",
       "  '0.154*\"sorry\" + 0.109*\"service\" + 0.096*\"time\" + 0.085*\"thank\" + 0.079*\"customer\" + 0.075*\"see\" + 0.052*\"amazon\" + 0.050*\"receive\" + 0.035*\"problem\" + 0.032*\"soon\"'),\n",
       " (9,\n",
       "  '0.231*\"us\" + 0.217*\"please\" + 0.096*\"thanks\" + 0.082*\"let\" + 0.082*\"know\" + 0.062*\"try\" + 0.062*\"work\" + 0.049*\"contact\" + 0.022*\"connect\" + 0.021*\"reply\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e00b43a0-a5fd-4346-a4ae-b51151aa8372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-04T06:14:31.091877Z",
     "iopub.status.busy": "2023-03-04T06:14:31.091765Z",
     "iopub.status.idle": "2023-03-04T06:14:36.581205Z",
     "shell.execute_reply": "2023-03-04T06:14:36.580647Z",
     "shell.execute_reply.started": "2023-03-04T06:14:31.091866Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 3:\n",
      "unawa (0.00%), speedwaybrick (0.00%), uttranscript (0.00%), bobus (0.00%), warde (0.00%), reallifestruggles (0.00%), inbond (0.00%), looreen (0.00%), aahhrrgh (0.00%), lyingdriver (0.00%)\n",
      "0.000*\"ledsham\" + 0.000*\"vilicent\" + 0.000*\"needtogethere\" + 0.000*\"deltacomfortplus\" + 0.000*\"nicefollowup\" + 0.000*\"roostiruez\" + 0.000*\"normie\" + 0.000*\"shitier\" + 0.000*\"carrioncrow\" + 0.000*\"freeandfabulous\"\n",
      "\n",
      "Topic 83:\n",
      "unawa (0.00%), speedwaybrick (0.00%), uttranscript (0.00%), bobus (0.00%), warde (0.00%), reallifestruggles (0.00%), inbond (0.00%), looreen (0.00%), aahhrrgh (0.00%), lyingdriver (0.00%)\n",
      "0.000*\"ledsham\" + 0.000*\"vilicent\" + 0.000*\"needtogethere\" + 0.000*\"deltacomfortplus\" + 0.000*\"nicefollowup\" + 0.000*\"roostiruez\" + 0.000*\"normie\" + 0.000*\"shitier\" + 0.000*\"carrioncrow\" + 0.000*\"freeandfabulous\"\n",
      "\n",
      "Topic 77:\n",
      "unawa (0.00%), speedwaybrick (0.00%), uttranscript (0.00%), bobus (0.00%), warde (0.00%), reallifestruggles (0.00%), inbond (0.00%), looreen (0.00%), aahhrrgh (0.00%), lyingdriver (0.00%)\n",
      "0.000*\"ledsham\" + 0.000*\"vilicent\" + 0.000*\"needtogethere\" + 0.000*\"deltacomfortplus\" + 0.000*\"nicefollowup\" + 0.000*\"roostiruez\" + 0.000*\"normie\" + 0.000*\"shitier\" + 0.000*\"carrioncrow\" + 0.000*\"freeandfabulous\"\n",
      "\n",
      "Topic 92:\n",
      "unawa (0.00%), speedwaybrick (0.00%), uttranscript (0.00%), bobus (0.00%), warde (0.00%), reallifestruggles (0.00%), inbond (0.00%), looreen (0.00%), aahhrrgh (0.00%), lyingdriver (0.00%)\n",
      "0.000*\"ledsham\" + 0.000*\"vilicent\" + 0.000*\"needtogethere\" + 0.000*\"deltacomfortplus\" + 0.000*\"nicefollowup\" + 0.000*\"roostiruez\" + 0.000*\"normie\" + 0.000*\"shitier\" + 0.000*\"carrioncrow\" + 0.000*\"freeandfabulous\"\n",
      "\n",
      "Topic 96:\n",
      "unawa (0.00%), speedwaybrick (0.00%), uttranscript (0.00%), bobus (0.00%), warde (0.00%), reallifestruggles (0.00%), inbond (0.00%), looreen (0.00%), aahhrrgh (0.00%), lyingdriver (0.00%)\n",
      "0.000*\"ledsham\" + 0.000*\"vilicent\" + 0.000*\"needtogethere\" + 0.000*\"deltacomfortplus\" + 0.000*\"nicefollowup\" + 0.000*\"roostiruez\" + 0.000*\"normie\" + 0.000*\"shitier\" + 0.000*\"carrioncrow\" + 0.000*\"freeandfabulous\"\n",
      "\n",
      "Topic 80:\n",
      "unawa (0.00%), speedwaybrick (0.00%), uttranscript (0.00%), bobus (0.00%), warde (0.00%), reallifestruggles (0.00%), inbond (0.00%), looreen (0.00%), aahhrrgh (0.00%), lyingdriver (0.00%)\n",
      "0.000*\"ledsham\" + 0.000*\"vilicent\" + 0.000*\"needtogethere\" + 0.000*\"deltacomfortplus\" + 0.000*\"nicefollowup\" + 0.000*\"roostiruez\" + 0.000*\"normie\" + 0.000*\"shitier\" + 0.000*\"carrioncrow\" + 0.000*\"freeandfabulous\"\n",
      "\n",
      "Topic 95:\n",
      "unawa (0.00%), speedwaybrick (0.00%), uttranscript (0.00%), bobus (0.00%), warde (0.00%), reallifestruggles (0.00%), inbond (0.00%), looreen (0.00%), aahhrrgh (0.00%), lyingdriver (0.00%)\n",
      "0.000*\"ledsham\" + 0.000*\"vilicent\" + 0.000*\"needtogethere\" + 0.000*\"deltacomfortplus\" + 0.000*\"nicefollowup\" + 0.000*\"roostiruez\" + 0.000*\"normie\" + 0.000*\"shitier\" + 0.000*\"carrioncrow\" + 0.000*\"freeandfabulous\"\n",
      "\n",
      "Topic 64:\n",
      "sorry (42.75%), hear (20.65%), item (9.80%), track (7.48%), question (6.88%), arrive (5.52%), open (4.86%), learn (1.71%), unawa (0.00%), speedwaybrick (0.00%)\n",
      "0.427*\"sorry\" + 0.206*\"hear\" + 0.098*\"item\" + 0.075*\"track\" + 0.069*\"question\" + 0.055*\"arrive\" + 0.049*\"open\" + 0.017*\"learn\" + 0.000*\"wontpayba\" + 0.000*\"vilicent\"\n",
      "wants her step brother home  im so worried, you dont even know. i hope he knows how much i care for him...\n",
      "\n",
      "Topic 32:\n",
      "get (74.93%), start (8.44%), touch (4.92%), together (3.98%), shit (3.02%), drive (2.66%), proceed (0.58%), weird (0.70%), pet (0.20%), goodbye (0.09%)\n",
      "0.749*\"get\" + 0.084*\"start\" + 0.049*\"touch\" + 0.040*\"together\" + 0.030*\"shit\" + 0.027*\"drive\" + 0.007*\"weird\" + 0.006*\"proceed\" + 0.002*\"pet\" + 0.001*\"goodbye\"\n",
      "I'm laying in bed facing the wall and trying to relax but I'm hearing so many things plus the air conditioning sound is so louad \n",
      "\n",
      "Topic 81:\n",
      "would (29.05%), like (35.81%), check (22.81%), right (9.77%), double (0.94%), key (0.53%), birthday (0.74%), unawa (0.00%), speedwaybrick (0.00%), uttranscript (0.00%)\n",
      "0.358*\"like\" + 0.291*\"would\" + 0.228*\"check\" + 0.098*\"right\" + 0.009*\"double\" + 0.007*\"birthday\" + 0.005*\"key\" + 0.000*\"ledsham\" + 0.000*\"boiiiiiiii\" + 0.000*\"wontpayba\"\n",
      "im lonely  keep me company! 22 female, california\n",
      "\n",
      "Topic 67:\n",
      "thanks (47.76%), work (27.84%), problem (9.87%), reply (8.72%), high (2.25%), quick (1.82%), macos (0.51%), sierra (0.58%), initial (0.25%), unawa (0.00%)\n",
      "0.478*\"thanks\" + 0.278*\"work\" + 0.099*\"problem\" + 0.087*\"reply\" + 0.023*\"high\" + 0.018*\"quick\" + 0.006*\"sierra\" + 0.005*\"macos\" + 0.002*\"initial\" + 0.000*\"wontpayba\"\n",
      "gosh it`t 9:39 am and i am soooo tired &quot;yawn&quot; i want to go back to sleep but i can`t \n",
      "\n",
      "Topic 97:\n",
      "help (63.88%), try (25.56%), area (4.16%), speed (2.61%), less (1.85%), demand (1.29%), maintain (0.22%), reliability (0.06%), unawa (0.00%), speedwaybrick (0.00%)\n",
      "0.639*\"help\" + 0.256*\"try\" + 0.042*\"area\" + 0.026*\"speed\" + 0.018*\"less\" + 0.013*\"demand\" + 0.002*\"maintain\" + 0.001*\"reliability\" + 0.000*\"shitier\" + 0.000*\"carrioncrow\"\n",
      ".. sad to hear about the discovery of the little girl from Tracy. Her poor family \n",
      "\n",
      "Topic 10:\n",
      "please (65.87%), name (9.21%), full (8.57%), address (10.30%), case (3.18%), refer (1.27%), small (0.83%), size (0.51%), unawa (0.00%), speedwaybrick (0.00%)\n",
      "0.659*\"please\" + 0.103*\"address\" + 0.092*\"name\" + 0.086*\"full\" + 0.032*\"case\" + 0.013*\"refer\" + 0.008*\"small\" + 0.005*\"size\" + 0.000*\"boiiiiiiii\" + 0.000*\"carrioncrow\"\n",
      "I need a holiday  only one day off this year.\n",
      "\n",
      "Topic 15:\n",
      "us (51.43%), let (16.51%), know (17.92%), assist (8.47%), due (2.87%), policy (0.96%), write (1.01%), holder (0.40%), privacy (0.25%), unawa (0.00%)\n",
      "0.514*\"us\" + 0.179*\"know\" + 0.165*\"let\" + 0.085*\"assist\" + 0.029*\"due\" + 0.010*\"write\" + 0.010*\"policy\" + 0.004*\"holder\" + 0.003*\"privacy\" + 0.000*\"farse\"\n",
      "lol, honeybaby, i sound like a ny-quil commercial. that word only looks right with a green background. i want to sleep but i can't \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "num_topic_words = 10\n",
    "\n",
    "def get_score(word, prob):\n",
    "    freq = word2freq(word)\n",
    "    if freq < 1. / local_size * 5:\n",
    "        return 0\n",
    "    return prob**2 / freq\n",
    "\n",
    "def print_topic(topic_index, model):\n",
    "    '''\n",
    "    Print topics most specific keywords (i.e. P(word|model) / P(word_global) or other specificity score)\n",
    "    '''\n",
    "    topic = model.get_topics()[topic_index]\n",
    "    weighted = []\n",
    "    for i, prob in enumerate(topic):\n",
    "        word = dictionary[i]\n",
    "        #weighted.append((word, prob / word2freq(word), prob))\n",
    "        #weighted.append((word, prob / (word2freq(word) - math.log2(word2freq(word))), prob))\n",
    "        #weighted.append((word, prob**2 / word2freq(word), prob))\n",
    "        weighted.append((word, get_score(word, prob), prob))\n",
    "    weighted.sort(key=lambda pair: pair[1], reverse=True)\n",
    "    print(f\"Topic {topic_index}:\")\n",
    "    top_keywords = [f\"{word} ({prob*100:.2f}%)\" for word, weight, prob in weighted[0:num_topic_words]]\n",
    "    print(', '.join(top_keywords))\n",
    "\n",
    "# Get topics as list of tuples of word and probability\n",
    "topics = lda_model.show_topics(num_top_topics + 4, num_words=1, formatted=False)\n",
    "\n",
    "for topic_index, topic in topics:\n",
    "    print(\"\")\n",
    "    print_topic(topic_index, lda_model)\n",
    "    print(lda_model.print_topic(topic_index))\n",
    "    best_line_idx, prob = representative_tweets[topic_index]\n",
    "    if best_line_idx >= 0:\n",
    "        print(lines[best_line_idx])\n",
    "        assert(best_topic(tokens[best_line_idx], lda_model)[0] == topic_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0985bf3-ddff-4986-af5b-048dc6083cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update LDA with new documents\n",
    "\n",
    "lda_model.update(corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26177d83-f8e0-47d1-a159-2f89b1669d60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T05:23:10.146725Z",
     "iopub.status.busy": "2023-03-03T05:23:10.146567Z",
     "iopub.status.idle": "2023-03-03T05:23:10.239105Z",
     "shell.execute_reply": "2023-03-03T05:23:10.238660Z",
     "shell.execute_reply.started": "2023-03-03T05:23:10.146712Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they killed off a character on one of my favorite shows and now i'm upset \n",
      "Topic 19:\n",
      "option (11.4%), charge (11.5%), report (8.6%), fee (4.3%), problem (13.3%), case (6.5%), gift (2.4%), great (14.2%), one (23.0%), bar (0.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.230*\"one\" + 0.142*\"great\" + 0.133*\"problem\" + 0.115*\"charge\" + 0.114*\"option\" + 0.086*\"report\" + 0.065*\"case\" + 0.043*\"fee\" + 0.024*\"gift\" + 0.015*\"job\"'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify new doc\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import sanitization\n",
    "\n",
    "line = lines[865]\n",
    "\n",
    "# Preprocess the new document\n",
    "[new_doc] = sanitization.sanitize_tokenize([line])\n",
    "\n",
    "# Get the topic distribution for the new document\n",
    "doc_topics = lda_model.get_document_topics(dictionary.doc2bow(new_doc))\n",
    "\n",
    "# Sort the topic distribution in descending order of probability\n",
    "doc_topics.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the index of the topic with the highest probability\n",
    "most_likely_topic = doc_topics[0][0]\n",
    "\n",
    "print(line)\n",
    "print_topic(most_likely_topic)\n",
    "lda_model.print_topic(most_likely_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03937cc9-f62f-43a9-918c-2acee0664114",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "With too many topics, multiple topics collapse to the same one.\n",
    "This is not overfitting, but a limitation of LDA's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04aeda-01ad-48fd-b305-35bfab516be7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract bigrams; word pairs which often go together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3a9b37-9cea-4ffb-8225-ace80a05080f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-02-28T07:25:11.749550Z",
     "iopub.status.busy": "2023-02-28T07:25:11.749442Z",
     "iopub.status.idle": "2023-02-28T07:25:11.770928Z",
     "shell.execute_reply": "2023-02-28T07:25:11.770501Z",
     "shell.execute_reply.started": "2023-02-28T07:25:11.749540Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminology:\n",
      "[('let', 'know'), ('email', 'address'), ('sorry', 'hear'), ('please', 'send'), ('customer', 'service'), ('take', 'look'), ('please', 'let'), ('direct', 'message'), ('help', 'send'), ('phone', 'number'), ('happy', 'help'), ('would', 'like'), ('help', 'please'), ('please', 'contact'), ('please', 'follow'), ('send', 'email'), ('send', 'note'), ('make', 'sure'), ('please', 'check'), ('look', 'like'), ('please', 'reach'), ('happy', 'halloween'), ('please', 'help'), ('need', 'help'), ('closer', 'look'), ('take', 'closer'), ('account', 'email'), ('anything', 'else'), ('could', 'help'), ('look', 'please'), ('need', 'assistance'), ('team', 'connect'), ('thanks', 'reaching'), ('contact', 'number'), ('hear', 'please'), ('please', 'provide'), ('confirmation', 'number'), ('could', 'please'), ('get', 'back'), ('help', 'hi')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "words = list(itertools.chain.from_iterable(tokens))  # concatenate list of lists into single list\n",
    "\n",
    "# Find the most common bigrams in the text\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "finder.apply_freq_filter(3)\n",
    "bigrams = finder.nbest(bigram_measures.raw_freq, 40)\n",
    "\n",
    "# Print the extracted terminology\n",
    "print(\"Terminology:\")\n",
    "print(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d5ae0-9a6b-48cd-82f2-d2e69199abf2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Findings:\n",
    "- Bigrams extract couplings of words which might be a better candidate for a topic than a single word\n",
    "- However, it's implementation work for minimal improvement\n",
    "- Leaving this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78d417-7134-415c-a48a-5458786c60fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a86e68a4-e74e-4f0a-8c18-bd292591ec1f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-01T13:01:54.803156Z",
     "iopub.status.busy": "2023-03-01T13:01:54.803017Z",
     "iopub.status.idle": "2023-03-01T13:01:55.033326Z",
     "shell.execute_reply": "2023-03-01T13:01:55.032653Z",
     "shell.execute_reply.started": "2023-03-01T13:01:54.803144Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1: \"understand would like assist would need get private secure link assist\" TF-IDF Vector:\n",
      "\n",
      "link 0.2163770131669878\n",
      "secured 0.4076803360531025\n",
      "private 0.3254862045213924\n",
      "get 0.16785541829098044\n",
      "need 0.19876920172398266\n",
      "assist 0.42952594902547303\n",
      "like 0.1856933517828438\n",
      "would 0.4012626207478553\n",
      "understand 0.25915693398943357\n",
      "115712 0.4076803360531025\n",
      "\n",
      "\n",
      "Tweet 2: \"propose\" TF-IDF Vector:\n",
      "\n",
      "propose 0.8626924156014288\n",
      "sprintcare 0.5057289749102494\n",
      "\n",
      "\n",
      "Tweet 3: \"send several private message one respond usual\" TF-IDF Vector:\n",
      "\n",
      "usual 0.435252488121658\n",
      "responding 0.393306048272026\n",
      "one 0.2386736509698984\n",
      "messages 0.36480363903287266\n",
      "several 0.3780932738873274\n",
      "sent 0.2712929625413191\n",
      "sprintcare 0.3249438025495913\n",
      "private 0.37904610162742797\n",
      "\n",
      "\n",
      "Tweet 4: \"please send us private message far assist click message top profile\" TF-IDF Vector:\n",
      "\n",
      "profile 0.36789227341541386\n",
      "top 0.3305398565133689\n",
      "click 0.31304490062567364\n",
      "message 0.47713555571699484\n",
      "us 0.14152121450761193\n",
      "send 0.19574471761291398\n",
      "please 0.1411333589215707\n",
      "private 0.3420478452852618\n",
      "assist 0.22569071026267432\n",
      "115712 0.4284242421800403\n",
      "\n",
      "\n",
      "Tweet 5: \"please send us private message gain far detail account\" TF-IDF Vector:\n",
      "\n",
      "account 0.22216087959843203\n",
      "details 0.23481614929772557\n",
      "gain 0.5055529752206038\n",
      "message 0.2642187135221172\n",
      "us 0.15673765153426372\n",
      "send 0.21679129482905593\n",
      "please 0.1563080935072071\n",
      "sprintcare 0.32475427085754394\n",
      "private 0.3788250134009663\n",
      "115712 0.474488646902012\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.corpus\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into TF-IDF vectors\n",
    "tfidf_vectors = vectorizer.fit_transform(lines)\n",
    "\n",
    "import scipy \n",
    "\n",
    "inverse_voc = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "\n",
    "# Print the first 5 TF-IDF vectors\n",
    "for i in range(5):\n",
    "    print(f\"Tweet {i+1}: \\\"{' '.join(tokens[i])}\\\" TF-IDF Vector:\\n\")\n",
    "    cx = scipy.sparse.coo_matrix(tfidf_vectors[i])\n",
    "    for i, j, v in zip(cx.row, cx.col, cx.data):\n",
    "        print(inverse_voc[j], v)\n",
    "    print('\\n')\n",
    "\n",
    "# tfidf_vectors[i] is a sparse matrix where only the words in one tweet get a score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5a411-abcf-47cb-9463-9a817d8174f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Weighted K-means clustering on word2vecs weighted by occurrence/global frequency or TF-IDF score\n",
    "Problems:\n",
    "- Doesn't take word co-occurrence into account\n",
    "- If we take the TF-IDF per tweet then longer tweets which often contain 'please help' will get lower scores,\n",
    "because the term freqs are lower because tweets are longer and because those words occur often, even though they are significant\n",
    "- Longer tweets get significantly lower scores. TF-IDF isn't good for separate tweets.\n",
    "- We don't have a proper way to represent the cluster by a single topic or just a couple of terms.\n",
    "\n",
    "In general we suffer from the problem that in order to distinguish current trends from usual we need data on what usual looks like. I've tried to fix this by comparing the tweets word frequency against a global word frequency obtained from the Brown corpus. However, that corpus is not really representative for twitter data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcddfc-892b-47f8-891f-bd25534c62fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T07:33:11.421871Z",
     "iopub.status.busy": "2023-03-01T07:33:11.421703Z",
     "iopub.status.idle": "2023-03-01T07:33:11.515999Z",
     "shell.execute_reply": "2023-03-01T07:33:11.515522Z",
     "shell.execute_reply.started": "2023-03-01T07:33:11.421858Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define the Word2Vec model parameters\n",
    "word2vec = Word2Vec(tokens, vector_size=100, window=15, min_count=1, workers=4)\n",
    "word2vec.save(\"word2vec.model\")\n",
    "\n",
    "> Use the extended token set which includes a large corpus of Brown. Use the cell below instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6124e80-7221-4310-837f-bd8fe92095d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(global_tokens, vector_size=256, window=15, min_count=1, workers=4)\n",
    "word2vec.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413184bf-6f7a-4fc6-95de-dc55ed7a658f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74635375-1f81-4007-832a-c741ee076b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T12:32:27.613702Z",
     "iopub.status.busy": "2023-03-01T12:32:27.613540Z",
     "iopub.status.idle": "2023-03-01T12:32:31.889983Z",
     "shell.execute_reply": "2023-03-01T12:32:31.889320Z",
     "shell.execute_reply.started": "2023-03-01T12:32:27.613688Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word2weight(word):\n",
    "    return 1. / (word2freq(word) + 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a6376-b067-4d8c-a432-ee2c02e804c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Main part of the method using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d301514-32cf-4612-bf1d-a9b3a075986e",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-01T09:24:16.809435Z",
     "iopub.status.busy": "2023-03-01T09:24:16.808852Z",
     "iopub.status.idle": "2023-03-01T09:24:17.166031Z",
     "shell.execute_reply": "2023-03-01T09:24:17.165501Z",
     "shell.execute_reply.started": "2023-03-01T09:24:16.809419Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(tfidf_vectors\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m     29\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mvocabulary_\n\u001b[0;32m---> 30\u001b[0m word_vecs \u001b[38;5;241m=\u001b[39m [word2vec\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocabulary\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Define the number of clusters\u001b[39;00m\n\u001b[1;32m     33\u001b[0m num_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(tfidf_vectors\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m     29\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mvocabulary_\n\u001b[0;32m---> 30\u001b[0m word_vecs \u001b[38;5;241m=\u001b[39m [\u001b[43mword2vec\u001b[49m\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocabulary\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Define the number of clusters\u001b[39;00m\n\u001b[1;32m     33\u001b[0m num_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import itertools\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "words = list(itertools.chain.from_iterable(tokens))  # concatenate list of lists into single list\n",
    "\n",
    "# Create the TF-IDF vectorizer and Fit it to the all tweets as one document\n",
    "stop_words = set()  # Stop words have already been filtered out from tokens!\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
    "tfidf_vectors = vectorizer.fit_transform([' '.join(words)])  # Join all tweets into single doc\n",
    "#tfidf_vectors = vectorizer.fit_transform([' '.join(token) for token in tokens]) \n",
    "\n",
    "# Get the vocabulary and the TF-IDF abs\n",
    "weights = np.squeeze(tfidf_vectors.toarray())\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "word_vecs = [word2vec.wv[word] for word in vocabulary.keys()]\n",
    "\n",
    "# Define the number of clusters\n",
    "num_clusters = 10\n",
    "\n",
    "# Create the KMeans object and fit it to the TF-IDF vectors\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(word_vecs, sample_weight=weights)\n",
    "\n",
    "# Get the cluster labels and the cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=10)\n",
    "neigh.fit(word_vecs)\n",
    "\n",
    "# Print the top words of each cluster, weighted by their TF-IDF score\n",
    "for i, center in enumerate(centers):\n",
    "    print(\"Cluster\", i+1, \"Top Words:\")\n",
    "    [dists], [indices] = neigh.kneighbors([center])\n",
    "    for word_index in indices:\n",
    "        word_weighted_score = weights[word_index]\n",
    "        word = word2vec.wv.index_to_key[word_index]\n",
    "        print(f\"\\t{word} (score: {word_weighted_score:.2e})\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89a1af-e0a0-44b9-9d7e-5233b0c07934",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Main part of the method using specificity score based on occurrence/global_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b536d7f-0665-43c1-bf37-20bf7a5eb070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T12:35:47.073496Z",
     "iopub.status.busy": "2023-03-01T12:35:47.073370Z",
     "iopub.status.idle": "2023-03-01T12:35:47.153730Z",
     "shell.execute_reply": "2023-03-01T12:35:47.152566Z",
     "shell.execute_reply.started": "2023-03-01T12:35:47.073485Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "twitter_fdist = FreqDist()\n",
    "for line in tokens:\n",
    "    for word in line:\n",
    "        twitter_fdist[word] += 1\n",
    "\n",
    "words = [word for word in twitter_fdist]\n",
    "\n",
    "weights = [word2weight(word) * fdist[word] for word in words]\n",
    "\n",
    "word_vecs = [word2vec.wv[word] for word in words]\n",
    "\n",
    "# Define the number of clusters\n",
    "num_clusters = 10\n",
    "\n",
    "# Create the KMeans object and fit it to the TF-IDF vectors\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(word_vecs, sample_weight=weights)\n",
    "\n",
    "# Get the cluster labels and the cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=50)\n",
    "neigh.fit(word_vecs)\n",
    "\n",
    "# Print the top words of each cluster, weighted by their specificity score\n",
    "for i, center in enumerate(centers):\n",
    "    print(\"Cluster\", i+1, \"Top Words:\")\n",
    "    [neighbor_indices] = neigh.kneighbors([center], return_distance=False)\n",
    "    sorted_neighbors = sorted(neighbor_indices, key=lambda i: - weights[i])\n",
    "    for word_index in sorted_neighbors[0:10]:  # get the 10 nearest neighbors with highest specificity scores\n",
    "        word_weighted_score = weights[word_index]\n",
    "        word = words[word_index]\n",
    "        print(f\"\\t{word} (score: {word_weighted_score:.2e})\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00f746-ea3d-4dc4-a7a4-7531636da87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Findings:\n",
    "- Resulting clusters seem kinda random.\n",
    "- Perhaps the word2vec mapping is a highly discontinuous space, meaning that the clusters span over multiple subjects.\n",
    "- Perhaps the clusters span a wide area of space and the words closest to the cluster center aren't good representatives of the set.\n",
    "- Perhaps the word2vec mapping just isn't that great; the idea is that similar / co-occurring words are mapped closer together,\n",
    "but it doesn't say anything about that unrelated words should be farther apart. There's a relevance cutoff beyond which \n",
    "the distance between mapped words doesn't mmean anything anymore.\n",
    "- Problem: this technique doesn't take significance into account. Any random word may happen to be at the center of a cluster even if it's not used often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec96c0-e3cd-4878-ad9e-5bee8fe068a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74989777-bcc9-4886-b4ed-8c5a1c6c6088",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# GAN with a Dirichlet latent space\n",
    "- Create a generator which converts points sampled from a Dirichlet distribution into random tweets.\n",
    "- Create a discriminator which predicts whether a given tweet is real.\n",
    "- Provide discriminator with real samples using word2vec as input.\n",
    "- Provide discriminator with fake samples from the generator.\n",
    "- Train network, including parameters of the Dirichlet distribution.\n",
    "\n",
    "### Feature extraction\n",
    "- Evaluate corners of the latent space, and optimize input location for the realness score of the discriminator, together with a penalty for longer tweets.\n",
    "- Compare all thusly obtained iconic tweets based on their realness score.\n",
    "- Take the top 10 as most prevalent topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86785371-e50c-478d-920e-5f86e8778e7f",
   "metadata": {},
   "source": [
    "# Hierarchical Dirichlet Process \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661a3fc-3070-4c14-a105-fab59238ffab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a3662-10f0-4cd7-94cd-a229889736aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "\n",
    "# Train HDP model\n",
    "hdp_model = HdpModel(corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345543d9-c410-43ee-9b5c-c96c7f3271dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print topics\n",
    "for topic in hdp_model.show_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22936bed-a8e5-472d-9ad9-8ecabb9f163e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T11:00:50.633685Z",
     "iopub.status.busy": "2023-03-03T11:00:50.633525Z",
     "iopub.status.idle": "2023-03-03T11:00:52.968665Z",
     "shell.execute_reply": "2023-03-03T11:00:52.968235Z",
     "shell.execute_reply.started": "2023-03-03T11:00:50.633672Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "us (1.94%), please (1.83%), order (0.80%), amazon (0.44%), help (1.14%), service (0.71%), contact (0.53%), delivery (0.42%), send (0.82%), hi (0.86%)\n",
      "0.019*\"us\" + 0.018*\"please\" + 0.012*\"get\" + 0.011*\"help\" + 0.009*\"sorry\" + 0.009*\"hi\" + 0.008*\"send\" + 0.008*\"order\" + 0.007*\"know\" + 0.007*\"service\"\n",
      "Topic 1:\n",
      "página (0.01%), happyoctober (0.01%), uncleanly (0.01%), sacramentoriver (0.01%), mentiona (0.01%), sportspak (0.01%), borjhar (0.01%), pancakesammmiright (0.01%), illogan (0.01%), alyhey (0.01%)\n",
      "0.011*\"us\" + 0.010*\"please\" + 0.008*\"get\" + 0.007*\"help\" + 0.005*\"hi\" + 0.005*\"thanks\" + 0.005*\"sorry\" + 0.004*\"send\" + 0.004*\"look\" + 0.004*\"know\"\n",
      "Topic 2:\n",
      "justcomcastthings (0.01%), kanno (0.01%), toomanysecondchances (0.01%), uthiru (0.01%), goingbacktoandroid (0.01%), otaylakesrd (0.01%), wuste (0.01%), zealandallblacks (0.01%), nocompensation (0.01%), صار (0.01%)\n",
      "0.010*\"us\" + 0.010*\"please\" + 0.008*\"get\" + 0.007*\"help\" + 0.005*\"hi\" + 0.005*\"thanks\" + 0.004*\"sorry\" + 0.004*\"send\" + 0.004*\"look\" + 0.004*\"know\"\n",
      "Topic 3:\n",
      "worthiness (0.01%), netboon (0.01%), altra (0.01%), isitdown (0.01%), zik (0.01%), unplayeable (0.01%), unbooked (0.01%), vijan (0.01%), amorosho (0.01%), xre (0.01%)\n",
      "0.010*\"us\" + 0.009*\"please\" + 0.008*\"get\" + 0.007*\"help\" + 0.005*\"hi\" + 0.005*\"thanks\" + 0.004*\"sorry\" + 0.004*\"send\" + 0.004*\"look\" + 0.004*\"know\"\n",
      "Topic 4:\n",
      "qwsyrk (0.01%), frameworthy (0.01%), preumium (0.01%), udey (0.01%), baldeu (0.01%), exactlt (0.01%), roughpatch (0.01%), reshipped (0.01%), policeto (0.01%), munudo (0.01%)\n",
      "0.010*\"us\" + 0.009*\"please\" + 0.008*\"get\" + 0.007*\"help\" + 0.005*\"hi\" + 0.005*\"thanks\" + 0.004*\"sorry\" + 0.004*\"send\" + 0.004*\"look\" + 0.004*\"know\"\n",
      "Topic 5:\n",
      "cmrc (0.01%), newphonefeels (0.01%), barbuda (0.01%), limplettuce (0.01%), princewills (0.01%), arrghhhhhh (0.01%), azuretrafficmanager (0.01%), wineadvent (0.01%), sailpost (0.01%), rudes (0.01%)\n",
      "0.010*\"us\" + 0.009*\"please\" + 0.008*\"get\" + 0.007*\"help\" + 0.005*\"hi\" + 0.005*\"thanks\" + 0.004*\"sorry\" + 0.004*\"send\" + 0.004*\"look\" + 0.004*\"know\"\n",
      "Topic 6:\n",
      "aaflight (0.01%), rasberrylemonade (0.01%), φmemomemoi (0.01%), roadwarriors (0.01%), skezza (0.01%), saleand (0.01%), tamô (0.01%), responsabiliti (0.01%), kymberlee (0.01%), cashrewards (0.01%)\n",
      "0.010*\"us\" + 0.009*\"please\" + 0.008*\"get\" + 0.006*\"help\" + 0.005*\"thanks\" + 0.005*\"hi\" + 0.004*\"sorry\" + 0.004*\"send\" + 0.004*\"look\" + 0.004*\"know\"\n",
      "Topic 7:\n",
      "promonycc (0.01%), expediate (0.01%), helpplease (0.01%), alreadynposted (0.01%), svartalfheim (0.01%), airteltigo (0.01%), leahn (0.01%), vegasshooting (0.01%), anotherdaygoesby (0.01%), regulateair (0.01%)\n",
      "0.010*\"us\" + 0.009*\"please\" + 0.008*\"get\" + 0.006*\"help\" + 0.005*\"hi\" + 0.005*\"thanks\" + 0.004*\"sorry\" + 0.004*\"send\" + 0.004*\"look\" + 0.004*\"know\"\n",
      "Topic 8:\n",
      "uwchlan (0.01%), sodiumoverload (0.01%), rresponding (0.01%), boldsky (0.01%), hoursall (0.01%), ohkkkk (0.01%), inss (0.01%), melissak (0.01%), activte (0.01%), fvcking (0.01%)\n",
      "0.010*\"us\" + 0.009*\"please\" + 0.008*\"get\" + 0.007*\"help\" + 0.005*\"hi\" + 0.005*\"thanks\" + 0.004*\"sorry\" + 0.004*\"send\" + 0.004*\"look\" + 0.004*\"know\"\n",
      "Topic 9:\n",
      "appcompat (0.01%), delivry (0.01%), implicates (0.01%), isons (0.01%), delayreplay (0.01%), mannings (0.01%), uhhhhhhhm (0.01%), wanderlots (0.01%), needthatroastbeeflife (0.01%), haveaword (0.01%)\n",
      "0.010*\"us\" + 0.009*\"please\" + 0.008*\"get\" + 0.007*\"help\" + 0.005*\"hi\" + 0.005*\"thanks\" + 0.004*\"sorry\" + 0.004*\"send\" + 0.004*\"look\" + 0.004*\"know\"\n"
     ]
    }
   ],
   "source": [
    "# Get topics as list of tuples of word and probability\n",
    "topics = hdp_model.show_topics(num_top_topics, num_words=0, formatted=False)\n",
    "\n",
    "for topic_index, topic in topics:\n",
    "    print_topic(topic_index, hdp_model)\n",
    "    print(hdp_model.print_topic(topic_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dad221-8273-479e-acb7-64b7eb39a09d",
   "metadata": {},
   "source": [
    "# Findings\n",
    "\n",
    "### How to display topics of LDA-type models?\n",
    "LDA type models give $P(word|topic)$, which inherently favors the display of more common words.\n",
    "\n",
    "We can use Bayes rule to get $P(topic|word)=P(word|topic)*P(topic)/P(word)$.\n",
    "(Note that ordering words by $P(topic|word)=P(word|topic)/P(word)$ for each topic is equivalent)\n",
    "This makes sense, because we would want to see the words which are most significant for that topic.\n",
    "However, that inherently favors the display of super uncommon words which happen to be present more in those specific topics. \n",
    "In NLP overfitting is a bigger problem because a large amount of posts have unique tokens in them.\n",
    "\n",
    "We could use some other formula to order to words for a topic, like \n",
    "\n",
    "$$\\frac{P(word|topic)}{P(word)^2 - log(P(word))} $$\n",
    "\n",
    "That would discourage very uncommon words from being displayed, while favoring less common words over more common words.\n",
    "However, this seems rather contrived.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
