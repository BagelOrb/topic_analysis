{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae1b77b-4ca7-4b27-b54d-236148f0ed63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T13:07:58.745027Z",
     "iopub.status.busy": "2023-03-01T13:07:58.744790Z",
     "iopub.status.idle": "2023-03-01T13:07:58.747451Z",
     "shell.execute_reply": "2023-03-01T13:07:58.747039Z",
     "shell.execute_reply.started": "2023-03-01T13:07:58.745009Z"
    },
    "tags": []
   },
   "source": [
    "# Load and clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa60414-fab0-4356-9c7d-4e149975e173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T13:19:03.557912Z",
     "iopub.status.busy": "2023-03-01T13:19:03.557738Z",
     "iopub.status.idle": "2023-03-01T13:21:22.155333Z",
     "shell.execute_reply": "2023-03-01T13:21:22.154871Z",
     "shell.execute_reply.started": "2023-03-01T13:19:03.557900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sanitization\n",
    "\n",
    "\n",
    "# Read the CSV file and extract the text data\n",
    "with open('ciphix NLP/translated_data.csv', 'r') as file:\n",
    "    lines_ = file.readlines()\n",
    "\n",
    "## temporarily use less than the full data set for faster computation\n",
    "# lines = lines_[0:len(lines_) // 10]\n",
    "lines = lines_\n",
    "print(len(lines))\n",
    "\n",
    "tokens = sanitization.sanitize_tokenize(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcdae3-ac8d-47c0-9e0f-ca5ee7df6b20",
   "metadata": {},
   "source": [
    "# Specificity score\n",
    "\n",
    "Make a score for each word based on how often it occurs in general speech, i.e. outside our current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbf5a2b-ef70-4ee9-a203-526a57e261ff",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-01T13:44:55.813809Z",
     "iopub.status.busy": "2023-03-01T13:44:55.813637Z",
     "iopub.status.idle": "2023-03-01T13:46:30.911593Z",
     "shell.execute_reply": "2023-03-01T13:46:30.911004Z",
     "shell.execute_reply.started": "2023-03-01T13:44:55.813796Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/tk/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/tk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Make a global corpus from Brown\n",
    "\n",
    "import nltk\n",
    "import gensim.models\n",
    "from nltk.corpus import brown\n",
    "\n",
    "try:\n",
    "    nltk.data.find('brown')\n",
    "except LookupError:\n",
    "    nltk.download('brown')\n",
    "\n",
    "global_tokens = sanitization.sanitize_tokenize(brown.words()) # Use brown corpus as single tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fcadfe2-bae2-471a-9832-9f9680051561",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-01T13:48:45.451694Z",
     "iopub.status.busy": "2023-03-01T13:48:45.451524Z",
     "iopub.status.idle": "2023-03-01T13:59:54.182473Z",
     "shell.execute_reply": "2023-03-01T13:59:54.181980Z",
     "shell.execute_reply.started": "2023-03-01T13:48:45.451682Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tk/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Add random twitter data to the curpus\n",
    "\n",
    "import csv\n",
    "\n",
    "# Read the CSV file and extract the text data\n",
    "lines = []\n",
    "#  Downloaded from https://www.kaggle.com/datasets/kazanova/sentiment140/discussion/60512\n",
    "with open('ciphix NLP/training.1600000.processed.noemoticon.csv', 'r', encoding='ISO-8859-1') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        lines.append(line[5])  # Column 5 is the actual tweet\n",
    "        \n",
    "rnd_twitter_tokens = sanitization.sanitize_tokenize(lines) \n",
    "\n",
    "global_tokens.extend(rnd_twitter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628bdfab-def8-41ea-bd2d-7b08480c1a01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T14:00:23.263968Z",
     "iopub.status.busy": "2023-03-01T14:00:23.263799Z",
     "iopub.status.idle": "2023-03-01T14:00:23.269903Z",
     "shell.execute_reply": "2023-03-01T14:00:23.269355Z",
     "shell.execute_reply.started": "2023-03-01T14:00:23.263955Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add our tokens to the set so we don't get problems with unknown tokens\n",
    "global_tokens.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb27b5c-85a4-4c71-ad47-a5bdc1b05238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T14:00:36.151980Z",
     "iopub.status.busy": "2023-03-01T14:00:36.151790Z",
     "iopub.status.idle": "2023-03-01T14:00:40.222030Z",
     "shell.execute_reply": "2023-03-01T14:00:40.221370Z",
     "shell.execute_reply.started": "2023-03-01T14:00:36.151967Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Create a frequency distribution of all words from a different context then our current dataset\n",
    "global_fdist = FreqDist()\n",
    "for line in global_tokens:\n",
    "    for word in line:\n",
    "        global_fdist[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b25a8c-fe38-4f48-891e-ac9785c5495c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T14:00:47.198807Z",
     "iopub.status.busy": "2023-03-01T14:00:47.198573Z",
     "iopub.status.idle": "2023-03-01T14:00:47.707757Z",
     "shell.execute_reply": "2023-03-01T14:00:47.707112Z",
     "shell.execute_reply.started": "2023-03-01T14:00:47.198792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('ciphix NLP/freqs.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for word in global_fdist:\n",
    "        writer.writerow([word, global_fdist.freq(word)])\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07767ffe-180d-4dab-81a9-04e844341757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T14:33:36.815462Z",
     "iopub.status.busy": "2023-03-01T14:33:36.815238Z",
     "iopub.status.idle": "2023-03-01T14:33:36.953373Z",
     "shell.execute_reply": "2023-03-01T14:33:36.952690Z",
     "shell.execute_reply.started": "2023-03-01T14:33:36.815446Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create mapping from words to their frequency in 'normal language'\n",
    "word2freq_dict = {}\n",
    "\n",
    "with open('ciphix NLP/freqs.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for [word, freq] in reader:\n",
    "        word2freq_dict[word] = float(freq)\n",
    "    \n",
    "def word2freq(word):\n",
    "    try: \n",
    "        return word2freq_dict[word]\n",
    "    except KeyError:  # word didn't occur in our dataset\n",
    "        return 1. / len(word2freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714d25c-6177-491d-b0af-67780cbc9a44",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de5df1c-3d5e-452d-8fb2-dd188aea1f58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T13:21:22.156122Z",
     "iopub.status.busy": "2023-03-01T13:21:22.155936Z",
     "iopub.status.idle": "2023-03-01T13:30:31.768412Z",
     "shell.execute_reply": "2023-03-01T13:30:31.767834Z",
     "shell.execute_reply.started": "2023-03-01T13:21:22.156108Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17,\n",
       "  '0.096*\"delay\" + 0.074*\"feedback\" + 0.062*\"much\" + 0.054*\"appreciate\" + 0.047*\"pass\" + 0.036*\"bag\" + 0.033*\"airport\" + 0.029*\"travel\"'),\n",
       " (3,\n",
       "  '0.077*\"card\" + 0.071*\"anything\" + 0.058*\"game\" + 0.040*\"ever\" + 0.039*\"play\" + 0.039*\"end\" + 0.038*\"else\" + 0.038*\"credit\"'),\n",
       " (2,\n",
       "  '0.202*\"team\" + 0.109*\"support\" + 0.076*\"connect\" + 0.054*\"touch\" + 0.053*\"report\" + 0.041*\"case\" + 0.040*\"note\" + 0.031*\"fill\"'),\n",
       " (5,\n",
       "  '0.096*\"ca\" + 0.082*\"change\" + 0.074*\"guy\" + 0.058*\"error\" + 0.045*\"code\" + 0.036*\"log\" + 0.034*\"battery\" + 0.030*\"expect\"'),\n",
       " (11,\n",
       "  '0.083*\"way\" + 0.055*\"free\" + 0.048*\"think\" + 0.047*\"feel\" + 0.043*\"someone\" + 0.039*\"something\" + 0.038*\"many\" + 0.029*\"everything\"'),\n",
       " (12,\n",
       "  '0.157*\"work\" + 0.093*\"app\" + 0.055*\"show\" + 0.051*\"device\" + 0.036*\"fly\" + 0.032*\"seem\" + 0.029*\"possible\" + 0.024*\"currently\"'),\n",
       " (13,\n",
       "  '0.162*\"service\" + 0.118*\"customer\" + 0.099*\"call\" + 0.039*\"offer\" + 0.036*\"package\" + 0.031*\"care\" + 0.029*\"people\" + 0.025*\"wrong\"'),\n",
       " (8,\n",
       "  '0.132*\"time\" + 0.049*\"even\" + 0.041*\"really\" + 0.039*\"since\" + 0.038*\"bad\" + 0.036*\"never\" + 0.035*\"last\" + 0.034*\"week\"'),\n",
       " (0,\n",
       "  '0.097*\"order\" + 0.060*\"day\" + 0.054*\"say\" + 0.047*\"delivery\" + 0.046*\"amazon\" + 0.037*\"today\" + 0.034*\"pay\" + 0.032*\"already\"'),\n",
       " (15,\n",
       "  '0.065*\"us\" + 0.060*\"please\" + 0.046*\"get\" + 0.041*\"help\" + 0.030*\"sorry\" + 0.027*\"thanks\" + 0.025*\"send\" + 0.025*\"hi\"')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "num_topics = 20\n",
    "num_top_topics = 10\n",
    "num_topic_words = 10\n",
    "\n",
    "# Create a dictionary of the tokens\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "\n",
    "# Create a corpus of the tokens\n",
    "corpus = [dictionary.doc2bow(line) for line in tokens]\n",
    "\n",
    "# Train an LDA model on the corpus\n",
    "lda_model = models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                     id2word=dictionary,\n",
    "                                     num_topics=num_topics,\n",
    "                                     passes=40,\n",
    "                                     alpha='auto',\n",
    "                                     random_state=42)\n",
    "\n",
    "# Print the most prevalent topics discussed in the text data\n",
    "lda_model.print_topics(num_topics=num_top_topics, num_words=8)\n",
    "    \n",
    "# Findings:\n",
    "#  Sometimes gives boorito, sometimes not, depending on random_state,`\n",
    "#  even though I think that term is highly specific and would be a good candidate for a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e00b43a0-a5fd-4346-a4ae-b51151aa8372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T14:26:22.811278Z",
     "iopub.status.busy": "2023-03-01T14:26:22.811077Z",
     "iopub.status.idle": "2023-03-01T14:26:23.610843Z",
     "shell.execute_reply": "2023-03-01T14:26:23.610369Z",
     "shell.execute_reply.started": "2023-03-01T14:26:22.811262Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "rebooking, guaranteed, downgrade, joanne, baggage, agent, feedback, delay, subscription, glitching\n",
      "Topic 1:\n",
      "dmed, tracking, autopay, spotify, faulty, verification, fortnite, rider, unlock, ensure\n",
      "Topic 2:\n",
      "godaddy, fraud, digit, hardwired, connect, chorizo, unhelpful, team, cancellation, needful\n",
      "Topic 3:\n",
      "divert, amzl, nrc, queso, instruction, flt, urgently, transaction, securely, hurricane\n",
      "Topic 4:\n",
      "gpu, hassle, gamertag, fraudulent, uninstalling, psn, reinstall, lhr, resolution, complaint\n",
      "Topic 5:\n",
      "fios, deactivate, cc, installation, deduction, device, patrickullmann, deactivation, app, refer\n",
      "Topic 6:\n",
      "usps, misleading, rep, airline, jfk, customer, preferred, package, service, fare\n",
      "Topic 7:\n",
      "deplane, rebook, lte, troubleshoot, notification, azhelp, doorbell, enable, october, constantly\n",
      "Topic 8:\n",
      "aateam, amazon, deliver, cox, delivery, uber, prime, driver, ubereats, incompetent\n",
      "Topic 9:\n",
      "glitchy, cust, kindly, detail, confirmation, provide, assistance, carrier, assist, reach\n"
     ]
    }
   ],
   "source": [
    "# Get topics as list of tuples of word and probability\n",
    "topics = lda_model.show_topics(num_top_topics, num_words=len(dictionary), formatted=False)\n",
    "\n",
    "# Weight topic word probabilities by specificity\n",
    "for i, (topic_index, topic) in enumerate(topics):\n",
    "    weighted = [(word, prob / word2freq(word)) for word, prob in topic]\n",
    "    weighted.sort(key=lambda pair: pair[1], reverse=True)\n",
    "    print(f\"Topic {i}:\")\n",
    "    top_keywords = [word for word, weight in weighted[0:num_topic_words]]\n",
    "    print(', '.join(top_keywords))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04aeda-01ad-48fd-b305-35bfab516be7",
   "metadata": {},
   "source": [
    "# Extract bigrams; word pairs which often go together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3a9b37-9cea-4ffb-8225-ace80a05080f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-02-28T07:25:11.749550Z",
     "iopub.status.busy": "2023-02-28T07:25:11.749442Z",
     "iopub.status.idle": "2023-02-28T07:25:11.770928Z",
     "shell.execute_reply": "2023-02-28T07:25:11.770501Z",
     "shell.execute_reply.started": "2023-02-28T07:25:11.749540Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminology:\n",
      "[('let', 'know'), ('email', 'address'), ('sorry', 'hear'), ('please', 'send'), ('customer', 'service'), ('take', 'look'), ('please', 'let'), ('direct', 'message'), ('help', 'send'), ('phone', 'number'), ('happy', 'help'), ('would', 'like'), ('help', 'please'), ('please', 'contact'), ('please', 'follow'), ('send', 'email'), ('send', 'note'), ('make', 'sure'), ('please', 'check'), ('look', 'like'), ('please', 'reach'), ('happy', 'halloween'), ('please', 'help'), ('need', 'help'), ('closer', 'look'), ('take', 'closer'), ('account', 'email'), ('anything', 'else'), ('could', 'help'), ('look', 'please'), ('need', 'assistance'), ('team', 'connect'), ('thanks', 'reaching'), ('contact', 'number'), ('hear', 'please'), ('please', 'provide'), ('confirmation', 'number'), ('could', 'please'), ('get', 'back'), ('help', 'hi')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "words = list(itertools.chain.from_iterable(tokens))  # concatenate list of lists into single list\n",
    "\n",
    "# Find the most common bigrams in the text\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "finder.apply_freq_filter(3)\n",
    "bigrams = finder.nbest(bigram_measures.raw_freq, 40)\n",
    "\n",
    "# Print the extracted terminology\n",
    "print(\"Terminology:\")\n",
    "print(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d5ae0-9a6b-48cd-82f2-d2e69199abf2",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "- Bigrams extract couplings of words which might be a better candidate for a topic than a single word\n",
    "- However, it's implementation work for minimal improvement\n",
    "- Leaving this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78d417-7134-415c-a48a-5458786c60fa",
   "metadata": {},
   "source": [
    "# Test of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a86e68a4-e74e-4f0a-8c18-bd292591ec1f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-01T13:01:54.803156Z",
     "iopub.status.busy": "2023-03-01T13:01:54.803017Z",
     "iopub.status.idle": "2023-03-01T13:01:55.033326Z",
     "shell.execute_reply": "2023-03-01T13:01:55.032653Z",
     "shell.execute_reply.started": "2023-03-01T13:01:54.803144Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1: \"understand would like assist would need get private secure link assist\" TF-IDF Vector:\n",
      "\n",
      "link 0.2163770131669878\n",
      "secured 0.4076803360531025\n",
      "private 0.3254862045213924\n",
      "get 0.16785541829098044\n",
      "need 0.19876920172398266\n",
      "assist 0.42952594902547303\n",
      "like 0.1856933517828438\n",
      "would 0.4012626207478553\n",
      "understand 0.25915693398943357\n",
      "115712 0.4076803360531025\n",
      "\n",
      "\n",
      "Tweet 2: \"propose\" TF-IDF Vector:\n",
      "\n",
      "propose 0.8626924156014288\n",
      "sprintcare 0.5057289749102494\n",
      "\n",
      "\n",
      "Tweet 3: \"send several private message one respond usual\" TF-IDF Vector:\n",
      "\n",
      "usual 0.435252488121658\n",
      "responding 0.393306048272026\n",
      "one 0.2386736509698984\n",
      "messages 0.36480363903287266\n",
      "several 0.3780932738873274\n",
      "sent 0.2712929625413191\n",
      "sprintcare 0.3249438025495913\n",
      "private 0.37904610162742797\n",
      "\n",
      "\n",
      "Tweet 4: \"please send us private message far assist click message top profile\" TF-IDF Vector:\n",
      "\n",
      "profile 0.36789227341541386\n",
      "top 0.3305398565133689\n",
      "click 0.31304490062567364\n",
      "message 0.47713555571699484\n",
      "us 0.14152121450761193\n",
      "send 0.19574471761291398\n",
      "please 0.1411333589215707\n",
      "private 0.3420478452852618\n",
      "assist 0.22569071026267432\n",
      "115712 0.4284242421800403\n",
      "\n",
      "\n",
      "Tweet 5: \"please send us private message gain far detail account\" TF-IDF Vector:\n",
      "\n",
      "account 0.22216087959843203\n",
      "details 0.23481614929772557\n",
      "gain 0.5055529752206038\n",
      "message 0.2642187135221172\n",
      "us 0.15673765153426372\n",
      "send 0.21679129482905593\n",
      "please 0.1563080935072071\n",
      "sprintcare 0.32475427085754394\n",
      "private 0.3788250134009663\n",
      "115712 0.474488646902012\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.corpus\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into TF-IDF vectors\n",
    "tfidf_vectors = vectorizer.fit_transform(lines)\n",
    "\n",
    "import scipy \n",
    "\n",
    "inverse_voc = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "\n",
    "# Print the first 5 TF-IDF vectors\n",
    "for i in range(5):\n",
    "    print(f\"Tweet {i+1}: \\\"{' '.join(tokens[i])}\\\" TF-IDF Vector:\\n\")\n",
    "    cx = scipy.sparse.coo_matrix(tfidf_vectors[i])\n",
    "    for i, j, v in zip(cx.row, cx.col, cx.data):\n",
    "        print(inverse_voc[j], v)\n",
    "    print('\\n')\n",
    "\n",
    "# tfidf_vectors[i] is a sparse matrix where only the words in one tweet get a score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5a411-abcf-47cb-9463-9a817d8174f8",
   "metadata": {},
   "source": [
    "# Weighted K-means clustering on word2vecs weighted by occurrence/global frequency or TF-IDF score\n",
    "Problems:\n",
    "- Doesn't take word co-occurrence into account\n",
    "- If we take the TF-IDF per tweet then longer tweets which often contain 'please help' will get lower scores,\n",
    "because the term freqs are lower because tweets are longer and because those words occur often, even though they are significant\n",
    "- Longer tweets get significantly lower scores. TF-IDF isn't good for separate tweets.\n",
    "- We don't have a proper way to represent the cluster by a single topic or just a couple of terms.\n",
    "\n",
    "In general we suffer from the problem that in order to distinguish current trends from usual we need data on what usual looks like. I've tried to fix this by comparing the tweets word frequency against a global word frequency obtained from the Brown corpus. However, that corpus is not really representative for twitter data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcddfc-892b-47f8-891f-bd25534c62fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T07:33:11.421871Z",
     "iopub.status.busy": "2023-03-01T07:33:11.421703Z",
     "iopub.status.idle": "2023-03-01T07:33:11.515999Z",
     "shell.execute_reply": "2023-03-01T07:33:11.515522Z",
     "shell.execute_reply.started": "2023-03-01T07:33:11.421858Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define the Word2Vec model parameters\n",
    "word2vec = Word2Vec(tokens, vector_size=100, window=15, min_count=1, workers=4)\n",
    "word2vec.save(\"word2vec.model\")\n",
    "\n",
    "> Use the extended token set which includes a large corpus of Brown. Use the cell below instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6124e80-7221-4310-837f-bd8fe92095d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(global_tokens, vector_size=256, window=15, min_count=1, workers=4)\n",
    "word2vec.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413184bf-6f7a-4fc6-95de-dc55ed7a658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74635375-1f81-4007-832a-c741ee076b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T12:32:27.613702Z",
     "iopub.status.busy": "2023-03-01T12:32:27.613540Z",
     "iopub.status.idle": "2023-03-01T12:32:31.889983Z",
     "shell.execute_reply": "2023-03-01T12:32:31.889320Z",
     "shell.execute_reply.started": "2023-03-01T12:32:27.613688Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word2weight(word):\n",
    "    return 1. / (global_fdist[word] + 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a6376-b067-4d8c-a432-ee2c02e804c8",
   "metadata": {},
   "source": [
    "## Main part of the method using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d301514-32cf-4612-bf1d-a9b3a075986e",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-01T09:24:16.809435Z",
     "iopub.status.busy": "2023-03-01T09:24:16.808852Z",
     "iopub.status.idle": "2023-03-01T09:24:17.166031Z",
     "shell.execute_reply": "2023-03-01T09:24:17.165501Z",
     "shell.execute_reply.started": "2023-03-01T09:24:16.809419Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(tfidf_vectors\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m     29\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mvocabulary_\n\u001b[0;32m---> 30\u001b[0m word_vecs \u001b[38;5;241m=\u001b[39m [word2vec\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocabulary\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Define the number of clusters\u001b[39;00m\n\u001b[1;32m     33\u001b[0m num_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(tfidf_vectors\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m     29\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mvocabulary_\n\u001b[0;32m---> 30\u001b[0m word_vecs \u001b[38;5;241m=\u001b[39m [\u001b[43mword2vec\u001b[49m\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocabulary\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Define the number of clusters\u001b[39;00m\n\u001b[1;32m     33\u001b[0m num_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import itertools\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "words = list(itertools.chain.from_iterable(tokens))  # concatenate list of lists into single list\n",
    "\n",
    "# Create the TF-IDF vectorizer and Fit it to the all tweets as one document\n",
    "stop_words = set()  # Stop words have already been filtered out from tokens!\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
    "tfidf_vectors = vectorizer.fit_transform([' '.join(words)])  # Join all tweets into single doc\n",
    "#tfidf_vectors = vectorizer.fit_transform([' '.join(token) for token in tokens]) \n",
    "\n",
    "# Get the vocabulary and the TF-IDF abs\n",
    "weights = np.squeeze(tfidf_vectors.toarray())\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "word_vecs = [word2vec.wv[word] for word in vocabulary.keys()]\n",
    "\n",
    "# Define the number of clusters\n",
    "num_clusters = 10\n",
    "\n",
    "# Create the KMeans object and fit it to the TF-IDF vectors\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(word_vecs, sample_weight=weights)\n",
    "\n",
    "# Get the cluster labels and the cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=10)\n",
    "neigh.fit(word_vecs)\n",
    "\n",
    "# Print the top words of each cluster, weighted by their TF-IDF score\n",
    "for i, center in enumerate(centers):\n",
    "    print(\"Cluster\", i+1, \"Top Words:\")\n",
    "    [dists], [indices] = neigh.kneighbors([center])\n",
    "    for word_index in indices:\n",
    "        word_weighted_score = weights[word_index]\n",
    "        word = word2vec.wv.index_to_key[word_index]\n",
    "        print(f\"\\t{word} (score: {word_weighted_score:.2e})\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89a1af-e0a0-44b9-9d7e-5233b0c07934",
   "metadata": {},
   "source": [
    "## Main part of the method using specificity score based on occurrence/global_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b536d7f-0665-43c1-bf37-20bf7a5eb070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T12:35:47.073496Z",
     "iopub.status.busy": "2023-03-01T12:35:47.073370Z",
     "iopub.status.idle": "2023-03-01T12:35:47.153730Z",
     "shell.execute_reply": "2023-03-01T12:35:47.152566Z",
     "shell.execute_reply.started": "2023-03-01T12:35:47.073485Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "twitter_fdist = FreqDist()\n",
    "for line in tokens:\n",
    "    for word in line:\n",
    "        twitter_fdist[word] += 1\n",
    "\n",
    "words = [word for word in twitter_fdist]\n",
    "\n",
    "weights = [word2weight(word) * fdist[word] for word in words]\n",
    "\n",
    "word_vecs = [word2vec.wv[word] for word in words]\n",
    "\n",
    "# Define the number of clusters\n",
    "num_clusters = 10\n",
    "\n",
    "# Create the KMeans object and fit it to the TF-IDF vectors\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(word_vecs, sample_weight=weights)\n",
    "\n",
    "# Get the cluster labels and the cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=50)\n",
    "neigh.fit(word_vecs)\n",
    "\n",
    "# Print the top words of each cluster, weighted by their specificity score\n",
    "for i, center in enumerate(centers):\n",
    "    print(\"Cluster\", i+1, \"Top Words:\")\n",
    "    [neighbor_indices] = neigh.kneighbors([center], return_distance=False)\n",
    "    sorted_neighbors = sorted(neighbor_indices, key=lambda i: - weights[i])\n",
    "    for word_index in sorted_neighbors[0:10]:  # get the 10 nearest neighbors with highest specificity scores\n",
    "        word_weighted_score = weights[word_index]\n",
    "        word = words[word_index]\n",
    "        print(f\"\\t{word} (score: {word_weighted_score:.2e})\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00f746-ea3d-4dc4-a7a4-7531636da87f",
   "metadata": {},
   "source": [
    "# Findings:\n",
    "- Resulting clusters seem kinda random.\n",
    "- Perhaps the word2vec mapping is a highly discontinuous space, meaning that the clusters span over multiple subjects.\n",
    "- Perhaps the clusters span a wide area of space and the words closest to the cluster center aren't good representatives of the set.\n",
    "- Perhaps the word2vec mapping just isn't that great; the idea is that similar / co-occurring words are mapped closer together,\n",
    "but it doesn't say anything about that unrelated words should be farther apart. There's a relevance cutoff beyond which \n",
    "the distance between mapped words doesn't mmean anything anymore.\n",
    "- Problem: this technique doesn't take significance into account. Any random word may happen to be at the center of a cluster even if it's not used often."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
