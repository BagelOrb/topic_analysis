{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35408030-120a-4bdb-9a29-3840c6b1b2a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T05:33:42.965501Z",
     "iopub.status.busy": "2023-02-28T05:33:42.965331Z",
     "iopub.status.idle": "2023-02-28T05:33:42.979696Z",
     "shell.execute_reply": "2023-02-28T05:33:42.979173Z",
     "shell.execute_reply.started": "2023-02-28T05:33:42.965488Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 TF-IDF Vector:   (0, 0)\t0.4532946552278861\n",
      "  (0, 1)\t0.7674945674619879\n",
      "  (0, 2)\t0.4532946552278861\n",
      "Document 2 TF-IDF Vector:   (0, 3)\t0.7674945674619879\n",
      "  (0, 0)\t0.4532946552278861\n",
      "  (0, 2)\t0.4532946552278861\n",
      "Document 3 TF-IDF Vector:   (0, 4)\t0.7674945674619879\n",
      "  (0, 0)\t0.4532946552278861\n",
      "  (0, 2)\t0.4532946552278861\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the NLTK stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Load the documents as a list of strings\n",
    "documents = [\"Insert your first document here\", \"Insert your second document here\", \"Insert your third document here\"]\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into TF-IDF vectors\n",
    "tfidf_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the TF-IDF vectors\n",
    "for i in range(len(documents)):\n",
    "    print(\"Document\", i+1, \"TF-IDF Vector:\", tfidf_vectors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b7017c-005c-4417-8eee-81c308e2aad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T07:20:03.878085Z",
     "iopub.status.busy": "2023-02-28T07:20:03.872822Z",
     "iopub.status.idle": "2023-02-28T07:20:04.622255Z",
     "shell.execute_reply": "2023-02-28T07:20:04.621399Z",
     "shell.execute_reply.started": "2023-02-28T07:20:03.877735Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Download the NLTK stopwords and lemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Helpers (TODO: move to separate file)\n",
    "import re\n",
    "\n",
    "url_re = re.compile(r'^https?://', re.IGNORECASE)\n",
    "\n",
    "\n",
    "def is_url(word):\n",
    "    return url_re.search(word) is not None\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file and extract the text data\n",
    "with open('ciphix NLP/translated_data.csv', 'r') as file:\n",
    "    text_ = file.readlines()\n",
    "\n",
    "    \n",
    "text_ = [line for line in text_ if not line.isspace()]  # remove empty lines\n",
    "\n",
    "## temporarily use less than the full data set for faster computation\n",
    "text = text_[0:len(text_) // 10]\n",
    "\n",
    "# Tokenize the text data and remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['u', 'dm'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = []\n",
    "for sentence in text:\n",
    "    # sentence = translator.translate(sentence, dest='en').text\n",
    "    # TODO: move into util function \\/\n",
    "    sentence = re.sub(r'[^\\s]+',\n",
    "                      lambda x: '' if '@' in x.group() or  # Remove @mentions and email@addresses.completely\n",
    "                                      '&' in x.group() or  # Remove special characters such as &amp\n",
    "                                      is_url(x.group()) else x.group(), sentence)  # Remove urls\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha()]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    tokens.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e0b129d-ebd7-4938-8e3f-6b4ff5c5cbbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T07:20:10.788020Z",
     "iopub.status.busy": "2023-02-28T07:20:10.787537Z",
     "iopub.status.idle": "2023-02-28T07:20:10.814404Z",
     "shell.execute_reply": "2023-02-28T07:20:10.813968Z",
     "shell.execute_reply.started": "2023-02-28T07:20:10.788005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['understand',\n",
       " 'would',\n",
       " 'like',\n",
       " 'assist',\n",
       " 'would',\n",
       " 'need',\n",
       " 'get',\n",
       " 'private',\n",
       " 'secured',\n",
       " 'link',\n",
       " 'assist']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
